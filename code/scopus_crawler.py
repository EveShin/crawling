import requests
import pandas as pd
import time
import re
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.service import Service
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class ScopusCrawler:
    def __init__(self, start_keyword_index=0, start_page=1):
        """Scopus í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”"""
        self.keywords = [
            "LLM OR large language model",
            "LLM embodied",
            "LLM AND IoT",
            "LLM wireless communications",
            "embodied AI AND IoT",
            "embodied AI internet of things",
            "LLM spectrum management",
            "embodied AI wireless communication",
        ]

        self.base_url = "https://www-scopus-com-ssl.oca.korea.ac.kr"
        self.library_url = "https://libs.korea.ac.kr/"  # ê³ ë ¤ëŒ€ ë„ì„œê´€ ì‚¬ì´íŠ¸
        self.driver = None
        self.results_data = {}

        # ì¬ì‹œì‘ ì„¤ì •
        self.start_keyword_index = start_keyword_index
        self.start_page = start_page

        print(
            f"ğŸ”„ ì‹œì‘ ì„¤ì •: í‚¤ì›Œë“œ {start_keyword_index + 1}ë²ˆì§¸ ('{self.keywords[start_keyword_index]}'), í˜ì´ì§€ {start_page}ë¶€í„°"
        )

    def setup_driver(self):
        """Chrome ë“œë¼ì´ë²„ ì„¤ì • (ë´‡ ê°ì§€ ìš°íšŒ ê°•í™”)"""
        chrome_options = Options()

        # ë´‡ ê°ì§€ ìš°íšŒë¥¼ ìœ„í•œ ê³ ê¸‰ ì„¤ì •
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option("useAutomationExtension", False)

        # ì‹¤ì œ ì‚¬ìš©ìì²˜ëŸ¼ ë³´ì´ê²Œ í•˜ëŠ” ì„¤ì •ë“¤
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-extensions")
        chrome_options.add_argument("--disable-plugins")
        chrome_options.add_argument(
            "--disable-images"
        )  # ì´ë¯¸ì§€ ë¡œë”© ë¹„í™œì„±í™”ë¡œ ì†ë„ í–¥ìƒ
        chrome_options.add_argument(
            "--disable-javascript"
        )  # ì¼ë¶€ ë´‡ ê°ì§€ ìŠ¤í¬ë¦½íŠ¸ ìš°íšŒ

        # ë¡œê·¸ ìˆ¨ê¸°ê¸°
        chrome_options.add_argument("--log-level=3")
        chrome_options.add_argument("--disable-logging")
        chrome_options.add_argument("--disable-background-networking")

        # ë” í˜„ì‹¤ì ì¸ User-Agent
        chrome_options.add_argument(
            "--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )

        # ìœˆë„ìš° í¬ê¸° ì„¤ì • (ë´‡ì€ ë³´í†µ headlessì´ë¯€ë¡œ ì‹¤ì œ í¬ê¸° ì„¤ì •)
        chrome_options.add_argument("--window-size=1920,1080")
        chrome_options.add_argument("--start-maximized")

        # ChromeDriver ìë™ ê´€ë¦¬
        service = Service(ChromeDriverManager().install())
        self.driver = webdriver.Chrome(service=service, options=chrome_options)

        # JavaScriptë¡œ webdriver ì†ì„± ìˆ¨ê¸°ê¸°
        self.driver.execute_script(
            "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"
        )

        # ì¶”ê°€ ë´‡ ê°ì§€ ìš°íšŒ ìŠ¤í¬ë¦½íŠ¸
        self.driver.execute_cdp_cmd(
            "Page.addScriptToEvaluateOnNewDocument",
            {
                "source": """
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => undefined
                });
                
                Object.defineProperty(navigator, 'plugins', {
                    get: () => [1, 2, 3, 4, 5]
                });
                
                Object.defineProperty(navigator, 'languages', {
                    get: () => ['en-US', 'en', 'ko-KR', 'ko']
                });
                
                window.chrome = {
                    runtime: {}
                };
            """
            },
        )

        logger.info("Chrome ë“œë¼ì´ë²„ ì„¤ì • ì™„ë£Œ (ë´‡ ê°ì§€ ìš°íšŒ ê°•í™”)")

    def extract_author_affiliation_mapping(self):
        """'Show all information' íŒ¨ë„ ê¸°ì¤€ ì €ì-ì†Œì† ë§¤í•‘ ì •ë³´ ì¶”ì¶œ"""
        author_affiliation_map = {}
        affiliation_dict = {}

        try:
            # âœ… 1. ì†Œì† ì •ë³´ ì¶”ì¶œ (ì²¨ì â†’ ì†Œì† í…ìŠ¤íŠ¸)
            print("ğŸ” ì†Œì† ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
            affiliation_elements = self.driver.find_elements(
                By.CSS_SELECTOR,
                "section[data-testid='detailed-information-affiliations'] ul.DetailedInformationFlyout_list__76Ipn li",
            )

            for aff in affiliation_elements:
                try:
                    sup = aff.find_element(By.TAG_NAME, "sup").text.strip()
                    text = aff.find_element(By.TAG_NAME, "span").text.strip()
                    affiliation_dict[sup] = text
                    print(f"ğŸ›ï¸ [{sup}] {text}")
                except Exception as e:
                    print(f"âš ï¸ ì†Œì† íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
                    continue

            # âœ… 2. ì €ì ì •ë³´ ì¶”ì¶œ (ì €ìëª… â†’ ì²¨ì ëª©ë¡)
            print("ğŸ‘¥ ì €ì ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
            author_elements = self.driver.find_elements(
                By.CSS_SELECTOR,
                "ul.DetailedInformationFlyout_list__76Ipn li[data-testid='authorItem-button']",
            )

            for i, author_el in enumerate(author_elements, 1):
                try:
                    name = author_el.find_element(
                        By.CSS_SELECTOR, "span.Button_text__0dddp"
                    ).text.strip()

                    # ì´ë©”ì¼ ìƒëµ í•„í„°ë§
                    if not name or name.lower() in ["authors"] or name.startswith("+"):
                        continue

                    superscripts = []
                    sup_elements = author_el.find_elements(
                        By.CSS_SELECTOR, "sup.AuthorList_affiliation__bTM3u"
                    )
                    for sup in sup_elements:
                        val = sup.text.strip()
                        for s in re.split(r"[,\s]+", val):
                            if s:
                                superscripts.append(s)

                    # ì²¨ìê°€ ì—†ìœ¼ë©´ ì „ì²´ ì†Œì† ì‚¬ìš©
                    if not superscripts and affiliation_dict:
                        superscripts = list(affiliation_dict.keys())

                    author_affiliation_map[name] = superscripts
                    print(f"ğŸ‘¤ {name} â†’ {superscripts}")

                except Exception as e:
                    print(f"âŒ ì €ì {i} ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
                    continue

        except Exception as e:
            print(f"âŒ ë§¤í•‘ ì „ì²´ ì‹¤íŒ¨: {str(e)}")

        return author_affiliation_map, affiliation_dict

    def human_like_delay(self, min_seconds=1, max_seconds=3):
        """ì‚¬ëŒì²˜ëŸ¼ ëœë¤í•œ ì§€ì—°ì‹œê°„"""
        import random

        delay = random.uniform(min_seconds, max_seconds)
        time.sleep(delay)

    def login_and_access_scopus(self):
        """ê³ ë ¤ëŒ€ ë„ì„œê´€ ë¡œê·¸ì¸ ë° Scopus ì ‘ê·¼"""
        try:
            # ê³ ë ¤ëŒ€ ë„ì„œê´€ ì‚¬ì´íŠ¸ë¡œ ì´ë™
            print("ğŸŒ ê³ ë ¤ëŒ€í•™êµ ë„ì„œê´€ ì‚¬ì´íŠ¸ë¡œ ì´ë™ ì¤‘...")
            self.driver.get(self.library_url)
            time.sleep(3)

            print("ğŸ“š ê³ ë ¤ëŒ€ ë„ì„œê´€ì— ë¡œê·¸ì¸ì„ ì™„ë£Œí•œ í›„ Enterë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”...")
            input("ê³ ë ¤ëŒ€ ë„ì„œê´€ ë¡œê·¸ì¸ ì™„ë£Œ í›„ Enter: ")

            # "í•™ìˆ DB" ë²„íŠ¼ í´ë¦­
            print("ğŸ” í•™ìˆ DB ë©”ë‰´ë¥¼ í´ë¦­í•©ë‹ˆë‹¤...")
            academic_db_button = WebDriverWait(self.driver, 10).until(
                EC.element_to_be_clickable(
                    (By.CSS_SELECTOR, "a[data-target='.home-service-link-group-2']")
                )
            )
            academic_db_button.click()
            time.sleep(2)

            # Scopus ë§í¬ í´ë¦­
            print("ğŸ“– Scopus ë°ì´í„°ë² ì´ìŠ¤ì— ì ‘ê·¼í•©ë‹ˆë‹¤...")
            scopus_link = WebDriverWait(self.driver, 10).until(
                EC.element_to_be_clickable(
                    (By.XPATH, "//a[contains(@href, 'scopus.com') and text()='Scopus']")
                )
            )

            # í˜„ì¬ ì°½ì—ì„œ Scopus ë§í¬ ì£¼ì†Œ ê°€ì ¸ì˜¤ê¸°
            scopus_url = scopus_link.get_attribute("href")
            print(f"ğŸ”— Scopus í˜ì´ì§€ë¡œ ì´ë™í•©ë‹ˆë‹¤...")

            # Scopusë¡œ ì´ë™
            self.driver.get(scopus_url)
            time.sleep(5)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°

            print("\n" + "=" * 60)
            print("ğŸ” Scopus ë¡œê·¸ì¸ ì•ˆë‚´")
            print("=" * 60)
            print("1. Scopus ë¡œê·¸ì¸ í˜ì´ì§€ê°€ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤")
            print("2. Scopus ê³„ì •ìœ¼ë¡œ ë¡œê·¸ì¸ì„ ì™„ë£Œí•´ì£¼ì„¸ìš”")
            print("3. ë¡œê·¸ì¸ í›„ Scopus ê²€ìƒ‰ í˜ì´ì§€ì— ì ‘ê·¼ë˜ë©´ Enterë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”")
            print("=" * 60)
            input("Scopus ë¡œê·¸ì¸ ì™„ë£Œ í›„ Enterë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”: ")

            print("âœ… Scopus ì ‘ê·¼ ì™„ë£Œ!")
            return True

        except Exception as e:
            logger.error(f"Scopus ì ‘ê·¼ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            print("âŒ ìë™ ì ‘ê·¼ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            print("ìˆ˜ë™ìœ¼ë¡œ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ì§„í–‰í•´ì£¼ì„¸ìš”:")
            print("1. ê³ ë ¤ëŒ€ ë„ì„œê´€ â†’ í•™ìˆ DB â†’ Scopus ì ‘ê·¼")
            print("2. Scopus ë¡œê·¸ì¸ ì™„ë£Œ")
            input("ìˆ˜ë™ìœ¼ë¡œ Scopus ì ‘ê·¼ ë° ë¡œê·¸ì¸ ì™„ë£Œ í›„ Enter: ")
            return True

    def set_results_per_page(self, count=200):
        """í˜ì´ì§€ë‹¹ í‘œì‹œí•  ê²°ê³¼ ìˆ˜ ì„¤ì •"""
        try:
            # Display ë“œë¡­ë‹¤ìš´ ì°¾ê¸°
            display_select = WebDriverWait(self.driver, 5).until(
                EC.presence_of_element_located(
                    (By.CSS_SELECTOR, ".Select-module__vDMww")
                )
            )

            from selenium.webdriver.support.ui import Select

            select = Select(display_select)
            select.select_by_value(str(count))

            time.sleep(3)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            logger.info(f"í˜ì´ì§€ë‹¹ {count}ê°œ ê²°ê³¼ë¡œ ì„¤ì • ì™„ë£Œ")

        except Exception as e:
            logger.warning(f"ê²°ê³¼ í‘œì‹œ ê°œìˆ˜ ì„¤ì • ì‹¤íŒ¨: {str(e)}")

    def search_keyword(self, keyword):
        """íŠ¹ì • í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ ì‹¤í–‰"""
        try:
            # í˜„ì¬ URL í™•ì¸ ë° ì •ë¦¬
            current_url = self.driver.current_url
            print(f"ğŸ”— í˜„ì¬ URL: {current_url}")

            # URL íŒŒì‹±í•˜ì—¬ ì˜¬ë°”ë¥¸ ê¸°ë³¸ URL êµ¬ì„±
            try:
                from urllib.parse import urlparse

                parsed_url = urlparse(current_url)

                # ê³ ë ¤ëŒ€ í”„ë¡ì‹œ URLë§Œ ì‚¬ìš©
                base_url = "https://www-scopus-com-ssl.oca.korea.ac.kr"

            except Exception as e:
                print(f"âŒ URL íŒŒì‹± ì˜¤ë¥˜: {e}")
                print(
                    "âš ï¸ Scopus í˜ì´ì§€ê°€ ì•„ë‹™ë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ Scopus ê²€ìƒ‰ í˜ì´ì§€ë¡œ ì´ë™í•´ì£¼ì„¸ìš”."
                )
                input("Scopus ê²€ìƒ‰ í˜ì´ì§€ ì ‘ê·¼ í›„ Enterë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”: ")
                base_url = "https://www-scopus-com-ssl.oca.korea.ac.kr"  # ê¸°ë³¸ê°’

            print(f"ğŸ”— ê¸°ë³¸ URL: {base_url}")

            # í˜„ì¬ í˜ì´ì§€ì—ì„œ ê²€ìƒ‰ í•„ë“œ ì°¾ê¸° ì‹œë„
            try:
                search_input = WebDriverWait(self.driver, 5).until(
                    EC.presence_of_element_located(
                        (
                            By.CSS_SELECTOR,
                            "input[placeholder=' '][class*='styleguide-input_input']",
                        )
                    )
                )
                print("âœ… í˜„ì¬ í˜ì´ì§€ì—ì„œ ê²€ìƒ‰ í•„ë“œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤!")

            except TimeoutException:
                # ê²€ìƒ‰ í˜ì´ì§€ë¡œ ì´ë™ ì‹œë„
                print("ğŸ” ê²€ìƒ‰ í˜ì´ì§€ë¡œ ì´ë™ì„ ì‹œë„í•©ë‹ˆë‹¤...")
                search_paths = [
                    "/search/form.uri?display=basic",
                    "/search/form.uri",
                    "/document/search.uri",
                ]

                for search_path in search_paths:
                    try:
                        search_url = base_url + search_path
                        print(f"ğŸ”— ì‹œë„í•˜ëŠ” URL: {search_url}")
                        self.driver.get(search_url)
                        time.sleep(3)

                        # ê²€ìƒ‰ í•„ë“œê°€ ìˆëŠ”ì§€ í™•ì¸
                        search_input = WebDriverWait(self.driver, 5).until(
                            EC.presence_of_element_located(
                                (
                                    By.CSS_SELECTOR,
                                    "input[placeholder=' '][class*='styleguide-input_input']",
                                )
                            )
                        )
                        print("âœ… ê²€ìƒ‰ í˜ì´ì§€ë¡œ ì´ë™ ì„±ê³µ!")
                        break
                    except Exception as e:
                        print(f"âŒ URL ì‹¤íŒ¨: {search_url} - {str(e)}")
                        continue
                else:
                    # ëª¨ë“  URL ì‹¤íŒ¨ì‹œ ìˆ˜ë™ ì´ë™ ìš”ì²­
                    print("âŒ ìë™ìœ¼ë¡œ ê²€ìƒ‰ í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    print("í˜„ì¬ í˜ì´ì§€ê°€ ì´ë¯¸ ê²€ìƒ‰ í˜ì´ì§€ë¼ë©´ ê·¸ëƒ¥ Enterë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
                    print(
                        "ì•„ë‹ˆë©´ ìˆ˜ë™ìœ¼ë¡œ Scopus ê²€ìƒ‰ í˜ì´ì§€ë¡œ ì´ë™í•œ í›„ Enterë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”."
                    )
                    input("Scopus ê²€ìƒ‰ í˜ì´ì§€ í™•ì¸ í›„ Enter: ")

                    # ë‹¤ì‹œ ê²€ìƒ‰ í•„ë“œ ì°¾ê¸° ì‹œë„
                    try:
                        search_input = WebDriverWait(self.driver, 10).until(
                            EC.presence_of_element_located(
                                (
                                    By.CSS_SELECTOR,
                                    "input[placeholder=' '][class*='styleguide-input_input']",
                                )
                            )
                        )
                    except:
                        print(
                            "âŒ ê²€ìƒ‰ í•„ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ ê²€ìƒ‰ì„ ì§„í–‰í•´ì£¼ì„¸ìš”."
                        )
                        print(f"ê²€ìƒ‰ì–´: {keyword}")
                        input("ìˆ˜ë™ ê²€ìƒ‰ ì™„ë£Œ í›„ Enter: ")
                        return True

            # ê²€ìƒ‰ì–´ ì…ë ¥
            print(f"ğŸ” ê²€ìƒ‰ì–´ ì…ë ¥: {keyword}")
            search_input.clear()
            self.human_like_delay(0.5, 1)  # íƒ€ì´í•‘ ì§€ì—°
            search_input.send_keys(keyword)

            # Search withinì„ Keywordsë¡œ ì„¤ì •
            try:
                search_within_dropdown = WebDriverWait(self.driver, 5).until(
                    EC.element_to_be_clickable(
                        (By.CSS_SELECTOR, "select[data-testid='select-search-within']")
                    )
                )

                from selenium.webdriver.support.ui import Select

                select = Select(search_within_dropdown)
                select.select_by_value("KEY")
                print("âœ… ê²€ìƒ‰ ë²”ìœ„ë¥¼ Keywordsë¡œ ì„¤ì •")
            except:
                print("âš ï¸ ê²€ìƒ‰ ë²”ìœ„ ì„¤ì •ì„ ê±´ë„ˆëœë‹ˆë‹¤.")

            # ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­
            print("ğŸ” ê²€ìƒ‰ ì‹¤í–‰...")
            self.human_like_delay(0.5, 1)  # í´ë¦­ ì „ ì§€ì—°
            search_button = WebDriverWait(self.driver, 5).until(
                EC.element_to_be_clickable(
                    (By.CSS_SELECTOR, "button[type='submit'][class*='Button_button']")
                )
            )
            search_button.click()

            # ê²°ê³¼ í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° (ì—¬ëŸ¬ selector ì‹œë„)
            result_selectors = [
                "tbody tr.TableItems-module__A6xTk",
                ".result-item",
                "[data-testid='search-results']",
                ".document-result",
                ".search-results-content",
            ]

            print("â³ ê²€ìƒ‰ ê²°ê³¼ ë¡œë”© ëŒ€ê¸°... (ìµœëŒ€ 15ì´ˆ)")
            for selector in result_selectors:
                try:
                    WebDriverWait(self.driver, 15).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, selector))
                    )
                    print(f"âœ… ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ ë¡œë”© ì™„ë£Œ!")
                    break
                except:
                    continue
            else:
                print("âš ï¸ ìë™ìœ¼ë¡œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                print("ê²€ìƒ‰ ê²°ê³¼ê°€ í‘œì‹œë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
                input("ê²€ìƒ‰ ê²°ê³¼ í™•ì¸ í›„ Enterë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”: ")

            # í˜ì´ì§€ë‹¹ ê²°ê³¼ ìˆ˜ë¥¼ 200ê°œë¡œ ì„¤ì • ì‹œë„
            self.set_results_per_page(200)

            logger.info(f"í‚¤ì›Œë“œ '{keyword}' ê²€ìƒ‰ ì™„ë£Œ")
            return True

        except Exception as e:
            logger.error(f"í‚¤ì›Œë“œ '{keyword}' ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            print(f"âŒ ìë™ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            print("ìˆ˜ë™ìœ¼ë¡œ ë‹¤ìŒì„ ì§„í–‰í•´ì£¼ì„¸ìš”:")
            print(f"1. ê²€ìƒ‰ì–´ '{keyword}' ì…ë ¥")
            print("2. Keywords ë²”ìœ„ë¡œ ê²€ìƒ‰")
            print("3. ê²€ìƒ‰ ê²°ê³¼ í™•ì¸ í›„ Enter")
            input("ìˆ˜ë™ ê²€ìƒ‰ ì™„ë£Œ í›„ Enter: ")
            return True

    def extract_paper_links(self, paper_elements):
        """ë…¼ë¬¸ ìš”ì†Œë“¤ì—ì„œ ìƒì„¸ í˜ì´ì§€ ë§í¬ë§Œ ì¶”ì¶œ"""
        paper_links = []

        for i, paper_element in enumerate(paper_elements, 1):
            try:
                # ë…¼ë¬¸ ìƒì„¸ í˜ì´ì§€ ë§í¬ë§Œ ì¶”ì¶œ
                title_link = paper_element.find_element(By.CSS_SELECTOR, "h3 a")
                href = title_link.get_attribute("href")

                # ì ˆëŒ€ URLì¸ì§€ ìƒëŒ€ URLì¸ì§€ í™•ì¸
                if href.startswith("http"):
                    paper_link = href
                else:
                    # ìƒëŒ€ URLì¸ ê²½ìš° ê¸°ë³¸ URLê³¼ ê²°í•©
                    if "oca.korea.ac.kr" in self.driver.current_url:
                        base_url = "https://www-scopus-com-ssl.oca.korea.ac.kr"
                    else:
                        base_url = "https://www.scopus.com"
                    paper_link = base_url + href

                paper_links.append(paper_link)
                print(f"ğŸ“„ [{i}] ë…¼ë¬¸ ë§í¬ ì¶”ì¶œ: {paper_link}")

            except Exception as e:
                logger.warning(f"ë…¼ë¬¸ ë§í¬ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                print(f"âŒ [{i}] ë…¼ë¬¸ ë§í¬ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
                continue

        return paper_links

    def get_detailed_author_info(self, paper_link):
        """ë…¼ë¬¸ ìƒì„¸ í˜ì´ì§€ì—ì„œ ì €ì ìƒì„¸ ì •ë³´ ì¶”ì¶œ (Show all information ë²„íŠ¼ í´ë¦­ í›„)"""
        detailed_info = {
            "authors": [],
            "emails": [],
            "detailed_affiliations": [],
            "universities": [],
            "countries": [],
            "link": paper_link,
        }

        try:
            # ìƒˆ íƒ­ì—ì„œ ìƒì„¸ í˜ì´ì§€ ì—´ê¸°
            self.driver.execute_script(f"window.open('{paper_link}', '_blank');")
            self.driver.switch_to.window(self.driver.window_handles[-1])

            # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            self.human_like_delay(3, 4)  # ì¶©ë¶„í•œ ì´ˆê¸° ë”œë ˆì´

            # "Show all information" ë²„íŠ¼ í´ë¦­
            try:
                show_all_button = WebDriverWait(self.driver, 10).until(
                    EC.element_to_be_clickable(
                        (By.XPATH, "//button[.//span[text()='Show all information']]")
                    )
                )
                self.driver.execute_script("arguments[0].click();", show_all_button)
                print("ğŸ” 'Show all information' ë²„íŠ¼ í´ë¦­ ì„±ê³µ ")
                time.sleep(5)  # ì¶©ë¶„í•œ ì‹œê°ì  í™•ì¸ ë”œë ˆì´
            except Exception as e:
                print(f"âš ï¸ 'Show all information' ë²„íŠ¼ í´ë¦­ ì‹¤íŒ¨ ë˜ëŠ” ì—†ìŒ: {str(e)}")

            # âœ… 'Show all information' íŒ¨ë„ ê¸°ì¤€ ì¶”ì¶œ
            author_affiliation_map, affiliation_dict = (
                self.extract_author_affiliation_mapping()
            )

            for name, superscripts in author_affiliation_map.items():
                detailed_info["authors"].append(name)

                # ì´ë©”ì¼ (í•´ë‹¹ author block ë‚´ì— ìˆì„ ê²½ìš°)
                try:
                    email_element = self.driver.find_element(
                        By.XPATH,
                        f"//span[text()='{name}']/ancestor::li//a[starts-with(@href, 'mailto:')]",
                    )
                    email = email_element.get_attribute("href").replace("mailto:", "")
                    detailed_info["emails"].append(email)
                except:
                    detailed_info["emails"].append("")

                # ì†Œì† ë§¤í•‘
                affs, univs, countries = [], [], []
                for sup in superscripts:
                    if sup in affiliation_dict:
                        aff_text = affiliation_dict[sup]
                        affs.append(aff_text)
                        parsed = self.parse_affiliation(aff_text)
                        univs.append(parsed["university"])
                        countries.append(parsed["country"])

                if not affs:
                    affs = [""]
                    univs = [""]
                    countries = [""]

                detailed_info["detailed_affiliations"].append(" | ".join(affs))
                detailed_info["universities"].append(" | ".join(univs))
                detailed_info["countries"].append(" | ".join(countries))

            self.driver.close()
            self.driver.switch_to.window(self.driver.window_handles[0])

        except Exception as e:
            logger.error(f"ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            try:
                if len(self.driver.window_handles) > 1:
                    self.driver.close()
                self.driver.switch_to.window(self.driver.window_handles[0])
            except:
                pass

        return detailed_info

    def parse_affiliation(self, affiliation_text):
        """ì†Œì† ì •ë³´ë¥¼ ì „ê³µ, ëŒ€í•™, êµ­ê°€ë¡œ ë¶„ë¦¬"""
        parsed = {"department": "", "university": "", "country": ""}

        if not affiliation_text:
            return parsed

        # ì‰¼í‘œë¡œ ë¶„ë¦¬
        parts = [part.strip() for part in affiliation_text.split(",")]

        if len(parts) >= 3:
            # ì¼ë°˜ì ì¸ í˜•íƒœ: Department, University, Country
            parsed["department"] = parts[0]
            parsed["university"] = parts[1]
            parsed["country"] = parts[-1]  # ë§ˆì§€ë§‰ì´ êµ­ê°€

        elif len(parts) == 2:
            # University, Country í˜•íƒœ
            parsed["university"] = parts[0]
            parsed["country"] = parts[1]

        elif len(parts) == 1:
            # ì „ì²´ë¥¼ ëŒ€í•™ìœ¼ë¡œ ê°„ì£¼
            parsed["university"] = parts[0]

        # University í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì§€ ì•Šì€ ì²« ë²ˆì§¸ partëŠ” í•™ê³¼ë¡œ ê°„ì£¼
        if (
            len(parts) >= 2
            and "university" not in parts[0].lower()
            and "college" not in parts[0].lower()
        ):
            for i, part in enumerate(parts[1:], 1):
                if any(
                    keyword in part.lower()
                    for keyword in ["university", "college", "institute", "school"]
                ):
                    parsed["department"] = parts[0]
                    parsed["university"] = part
                    if i + 1 < len(parts):
                        parsed["country"] = parts[-1]
                    break

        print(f"ğŸ“ ì†Œì† ë¶„ì„: {affiliation_text}")
        print(f"   - ì „ê³µ: {parsed['department']}")
        print(f"   - ëŒ€í•™: {parsed['university']}")
        print(f"   - êµ­ê°€: {parsed['country']}")

        return parsed

    def save_batch_results(
        self, keyword, papers_data, start_page, end_page, paper_start_index=1
    ):
        """ë°°ì¹˜ ê²°ê³¼ ì €ì¥ (5í˜ì´ì§€ì”©) - ë…¼ë¬¸ë³„ ë²ˆí˜¸ ì¶”ê°€ ë° êµ¬ë¶„"""
        try:
            # ì•ˆì „í•œ íŒŒì¼ëª… ìƒì„±
            safe_keyword = re.sub(r"[^\w\s-]", "", keyword).replace(" ", "_")
            filename = f"scopus_{safe_keyword}_pages_{start_page}-{end_page}.xlsx"

            if papers_data:
                # ë°ì´í„° ì •ë¦¬
                formatted_data = []
                current_paper_index = paper_start_index

                for paper in papers_data:
                    authors = paper.get("authors", [""])
                    emails = paper.get("emails", [""])
                    affiliations = paper.get("detailed_affiliations", [""])
                    universities = paper.get("universities", [""])
                    countries = paper.get("countries", [""])

                    max_len = max(
                        len(authors),
                        len(emails),
                        len(affiliations),
                        len(universities),
                        len(countries),
                    )
                    authors.extend([""] * (max_len - len(authors)))
                    emails.extend([""] * (max_len - len(emails)))
                    affiliations.extend([""] * (max_len - len(affiliations)))
                    universities.extend([""] * (max_len - len(universities)))
                    countries.extend([""] * (max_len - len(countries)))

                    for i in range(max_len):
                        formatted_data.append(
                            {
                                "ë…¼ë¬¸ë²ˆí˜¸": (
                                    current_paper_index if i == 0 else ""
                                ),  # ì²« ë²ˆì§¸ ì €ìì—ë§Œ ë…¼ë¬¸ë²ˆí˜¸ í‘œì‹œ
                                "ì €ì": authors[i] if i < len(authors) else "",
                                "ì´ë©”ì¼": emails[i] if i < len(emails) else "",
                                "ì†Œì†(ì „ê³µ)": (
                                    affiliations[i] if i < len(affiliations) else ""
                                ),
                                "ì†Œì†(ëŒ€í•™)": (
                                    universities[i] if i < len(universities) else ""
                                ),
                                "ì†Œì†(êµ­ê°€)": (
                                    countries[i] if i < len(countries) else ""
                                ),
                                "ë…¼ë¬¸ ë§í¬": (
                                    paper.get("link", "") if i == 0 else ""
                                ),  # ì²« ë²ˆì§¸ ì €ìì—ë§Œ ë§í¬ í‘œì‹œ
                            }
                        )

                    # ë…¼ë¬¸ êµ¬ë¶„ì„ ìœ„í•œ ë¹ˆ í–‰ ì¶”ê°€
                    formatted_data.append(
                        {
                            "ë…¼ë¬¸ë²ˆí˜¸": "",
                            "ì €ì": "",
                            "ì´ë©”ì¼": "",
                            "ì†Œì†(ì „ê³µ)": "",
                            "ì†Œì†(ëŒ€í•™)": "",
                            "ì†Œì†(êµ­ê°€)": "",
                            "ë…¼ë¬¸ ë§í¬": "",
                        }
                    )

                    current_paper_index += 1

                df = pd.DataFrame(formatted_data)
                df.to_excel(filename, index=False)
                print(
                    f"ğŸ’¾ ë°°ì¹˜ ì €ì¥ ì™„ë£Œ: {filename} ({len(papers_data)}ê°œ ë…¼ë¬¸, {start_page}-{end_page}í˜ì´ì§€)"
                )

                return current_paper_index  # ë‹¤ìŒ ë…¼ë¬¸ ë²ˆí˜¸ ë°˜í™˜

        except Exception as e:
            print(f"âŒ ë°°ì¹˜ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            return paper_start_index

    def crawl_pages(self, keyword, max_pages=200, start_page=1):
        """íŠ¹ì • í‚¤ì›Œë“œì— ëŒ€í•´ ì—¬ëŸ¬ í˜ì´ì§€ í¬ë¡¤ë§"""
        papers_data = []
        batch_papers = []  # 5í˜ì´ì§€ì”© ëª¨ì„ ì„ì‹œ ì €ì¥ì†Œ

        # ë°°ì¹˜ ì‹œì‘ í˜ì´ì§€ ê³„ì‚° (5ì˜ ë°°ìˆ˜ë¡œ ì¡°ì •)
        batch_start_page = ((start_page - 1) // 5) * 5 + 1

        # ë…¼ë¬¸ ë²ˆí˜¸ ì‹œì‘ê°’ ê³„ì‚° (í˜ì´ì§€ë‹¹ í‰ê·  ë…¼ë¬¸ ìˆ˜ë¥¼ ê³ ë ¤)
        paper_index = ((start_page - 1) * 20) + 1  # í˜ì´ì§€ë‹¹ ëŒ€ëµ 20ê°œ ë…¼ë¬¸ìœ¼ë¡œ ì¶”ì •

        # ì²« ê²€ìƒ‰ ì‹¤í–‰
        if not self.search_keyword(keyword):
            return papers_data

        # ì‹œì‘ í˜ì´ì§€ê°€ 1ì´ ì•„ë‹ˆë©´ í•´ë‹¹ í˜ì´ì§€ë¡œ ì´ë™
        if start_page > 1:
            print(f"ğŸ”„ í˜ì´ì§€ {start_page}ë¡œ ì´ë™ ì¤‘...")
            success = self.navigate_to_page(start_page)
            if not success:
                print(f"âŒ í˜ì´ì§€ {start_page}ë¡œ ì´ë™ ì‹¤íŒ¨. í˜ì´ì§€ 1ë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.")
                start_page = 1
                batch_start_page = 1
                paper_index = 1

        for page_num in range(start_page, max_pages + 1):
            try:
                logger.info(
                    f"í‚¤ì›Œë“œ '{keyword}' - í˜ì´ì§€ {page_num}/{max_pages} í¬ë¡¤ë§ ì¤‘..."
                )

                # í˜„ì¬ í˜ì´ì§€ì˜ ë…¼ë¬¸ ë§í¬ë“¤ ìˆ˜ì§‘
                paper_elements = WebDriverWait(self.driver, 10).until(
                    EC.presence_of_all_elements_located(
                        (By.CSS_SELECTOR, "tbody tr.TableItems-module__A6xTk")
                    )
                )

                if not paper_elements:
                    logger.info(f"í˜ì´ì§€ {page_num}ì—ì„œ ë” ì´ìƒ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    break

                print(f"ğŸ“‹ í˜ì´ì§€ {page_num}: {len(paper_elements)}ê°œ ë…¼ë¬¸ ë°œê²¬")

                # ë…¼ë¬¸ ë§í¬ë“¤ ì¶”ì¶œ
                paper_links = self.extract_paper_links(paper_elements)

                # ê° ë…¼ë¬¸ ìƒì„¸ í˜ì´ì§€ì—ì„œ ì €ì ì •ë³´ ì¶”ì¶œ
                page_papers = []
                for j, paper_link in enumerate(paper_links, 1):
                    print(
                        f"ğŸ” [{j}/{len(paper_links)}] ë…¼ë¬¸ {paper_index}ë²ˆ ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì¤‘..."
                    )

                    # ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                    detailed_info = self.get_detailed_author_info(paper_link)
                    detailed_info["link"] = paper_link
                    detailed_info["paper_number"] = paper_index  # ë…¼ë¬¸ ë²ˆí˜¸ ì¶”ê°€

                    page_papers.append(detailed_info)
                    papers_data.append(detailed_info)
                    batch_papers.append(detailed_info)

                    print(
                        f"âœ… ë…¼ë¬¸ {paper_index}ë²ˆ: ì €ì {len(detailed_info.get('authors', []))}ëª…, ì´ë©”ì¼ {len(detailed_info.get('emails', []))}ê°œ ìˆ˜ì§‘"
                    )

                    paper_index += 1
                    self.human_like_delay(2, 4)

                # 5í˜ì´ì§€ë§ˆë‹¤ ë˜ëŠ” ë§ˆì§€ë§‰ í˜ì´ì§€ì—ì„œ ë°°ì¹˜ ì €ì¥
                if page_num % 5 == 0 or page_num == max_pages:
                    if batch_papers:
                        batch_end_page = page_num
                        # ë…¼ë¬¸ ë²ˆí˜¸ ì‹œì‘ê°’ ê³„ì‚°
                        batch_paper_start_index = batch_papers[0]["paper_number"]
                        self.save_batch_results(
                            keyword,
                            batch_papers,
                            batch_start_page,
                            batch_end_page,
                            batch_paper_start_index,
                        )

                        # ì§„í–‰ ìƒí™© ì €ì¥
                        self.save_progress(keyword, page_num + 1)

                        # ë°°ì¹˜ ì´ˆê¸°í™”
                        batch_papers = []
                        batch_start_page = page_num + 1

                print(f"ğŸ“„ í˜ì´ì§€ {page_num} ì™„ë£Œ: {len(page_papers)}ê°œ ë…¼ë¬¸ ìˆ˜ì§‘")

                # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
                try:
                    next_button = self.driver.find_element(
                        By.XPATH, "//button[.//span[text()='Next']]"
                    )

                    if next_button.is_enabled() and not next_button.get_attribute(
                        "disabled"
                    ):
                        next_button.click()
                        self.human_like_delay(3, 5)
                        logger.info(f"í˜ì´ì§€ {page_num + 1}ë¡œ ì´ë™")
                    else:
                        logger.info("ë” ì´ìƒ ë‹¤ìŒ í˜ì´ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
                        # ë§ˆì§€ë§‰ì— ë‚¨ì€ ë°°ì¹˜ ì €ì¥
                        if batch_papers:
                            batch_paper_start_index = batch_papers[0]["paper_number"]
                            self.save_batch_results(
                                keyword,
                                batch_papers,
                                batch_start_page,
                                page_num,
                                batch_paper_start_index,
                            )
                        break

                except NoSuchElementException:
                    logger.info(
                        "Next ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ í˜ì´ì§€ì¸ ê²ƒ ê°™ìŠµë‹ˆë‹¤."
                    )
                    # ë§ˆì§€ë§‰ì— ë‚¨ì€ ë°°ì¹˜ ì €ì¥
                    if batch_papers:
                        batch_paper_start_index = batch_papers[0]["paper_number"]
                        self.save_batch_results(
                            keyword,
                            batch_papers,
                            batch_start_page,
                            page_num,
                            batch_paper_start_index,
                        )
                    break
                except Exception as e:
                    logger.error(f"ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    # ì˜¤ë¥˜ ë°œìƒì‹œì—ë„ ë°°ì¹˜ ì €ì¥
                    if batch_papers:
                        batch_paper_start_index = batch_papers[0]["paper_number"]
                        self.save_batch_results(
                            keyword,
                            batch_papers,
                            batch_start_page,
                            page_num,
                            batch_paper_start_index,
                        )
                    break

            except Exception as e:
                logger.error(f"í˜ì´ì§€ {page_num} í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                continue

        # ë§ˆì§€ë§‰ì— ë‚¨ì€ ë°°ì¹˜ê°€ ìˆë‹¤ë©´ ì €ì¥
        if batch_papers:
            batch_paper_start_index = batch_papers[0]["paper_number"]
            self.save_batch_results(
                keyword,
                batch_papers,
                batch_start_page,
                page_num,
                batch_paper_start_index,
            )

        logger.info(f"í‚¤ì›Œë“œ '{keyword}' í¬ë¡¤ë§ ì™„ë£Œ: {len(papers_data)}ê°œ ë…¼ë¬¸")
        return papers_data

    def navigate_to_page(self, target_page):
        """íŠ¹ì • í˜ì´ì§€ë¡œ ì´ë™"""
        try:
            print(f"ğŸ“„ í˜ì´ì§€ {target_page}ë¡œ ì´ë™ ì¤‘...")

            # ê°„ë‹¨í•œ ë°©ë²•: target_page - 1ë²ˆ Next ë²„íŠ¼ í´ë¦­
            for i in range(target_page - 1):
                try:
                    next_button = self.driver.find_element(
                        By.XPATH, "//button[.//span[text()='Next']]"
                    )
                    if next_button.is_enabled():
                        next_button.click()
                        self.human_like_delay(2, 3)
                        print(f"  -> í˜ì´ì§€ {i + 2}ë¡œ ì´ë™")
                    else:
                        print(f"âŒ í˜ì´ì§€ {i + 2}ë¡œ ì´ë™ ì‹¤íŒ¨")
                        return False
                except:
                    print(f"âŒ í˜ì´ì§€ {i + 2}ë¡œ ì´ë™ ì¤‘ ì˜¤ë¥˜")
                    return False

            print(f"âœ… í˜ì´ì§€ {target_page} ë„ì°©")
            return True

        except Exception as e:
            print(f"âŒ í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨: {str(e)}")
            return False

    def save_progress(self, keyword, next_page):
        """ì§„í–‰ ìƒí™© ì €ì¥"""
        try:
            progress_info = {
                "keyword": keyword,
                "keyword_index": self.keywords.index(keyword),
                "next_page": next_page,
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            }

            with open("scopus_progress.txt", "w", encoding="utf-8") as f:
                f.write(f"ë§ˆì§€ë§‰ ì™„ë£Œ: {progress_info['keyword']}\n")
                f.write(f"í‚¤ì›Œë“œ ë²ˆí˜¸: {progress_info['keyword_index']}\n")
                f.write(f"ë‹¤ìŒ ì‹œì‘ í˜ì´ì§€: {progress_info['next_page']}\n")
                f.write(f"ì‹œê°„: {progress_info['timestamp']}\n")
                f.write(f"\nì¬ì‹œì‘ ë°©ë²•:\n")
                f.write(
                    f"crawler = ScopusCrawler(start_keyword_index={progress_info['keyword_index']}, start_page={progress_info['next_page']})\n"
                )

        except Exception as e:
            print(f"âŒ ì§„í–‰ ìƒí™© ì €ì¥ ì‹¤íŒ¨: {str(e)}")

    def save_to_excel(self, filename="scopus_papers.xlsx"):
        """ê²°ê³¼ë¥¼ ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥ - ë…¼ë¬¸ë³„ ë²ˆí˜¸ ë° êµ¬ë¶„ ì¶”ê°€"""
        with pd.ExcelWriter(filename, engine="openpyxl") as writer:
            for keyword, papers_data in self.results_data.items():
                if papers_data:
                    # ë°ì´í„° ì •ë¦¬
                    formatted_data = []

                    for paper_index, paper in enumerate(papers_data, 1):
                        # ì €ìë³„ë¡œ í–‰ ë¶„ë¦¬
                        authors = paper.get("authors", [""])
                        emails = paper.get("emails", [""])
                        detailed_affiliations = paper.get(
                            "detailed_affiliations", [""]
                        )  # ì›ë³¸ ì†Œì†
                        universities = paper.get("universities", [""])  # íŒŒì‹±ëœ ëŒ€í•™
                        countries = paper.get("countries", [""])  # íŒŒì‹±ëœ êµ­ê°€

                        # ë¦¬ìŠ¤íŠ¸ ê¸¸ì´ ë§ì¶”ê¸°
                        max_len = max(
                            len(authors),
                            len(emails),
                            len(detailed_affiliations),
                            len(universities),
                            len(countries),
                        )
                        authors.extend([""] * (max_len - len(authors)))
                        emails.extend([""] * (max_len - len(emails)))
                        detailed_affiliations.extend(
                            [""] * (max_len - len(detailed_affiliations))
                        )
                        universities.extend([""] * (max_len - len(universities)))
                        countries.extend([""] * (max_len - len(countries)))

                        for i in range(max_len):
                            formatted_data.append(
                                {
                                    "ë…¼ë¬¸ë²ˆí˜¸": (
                                        paper_index if i == 0 else ""
                                    ),  # ì²« ë²ˆì§¸ ì €ìì—ë§Œ ë…¼ë¬¸ë²ˆí˜¸ í‘œì‹œ
                                    "ì €ì": authors[i] if i < len(authors) else "",
                                    "ì´ë©”ì¼": emails[i] if i < len(emails) else "",
                                    "ì†Œì†(ì „ê³µ)": (
                                        detailed_affiliations[i]
                                        if i < len(detailed_affiliations)
                                        else ""
                                    ),  # ì›ë³¸ ì†Œì†
                                    "ì†Œì†(ëŒ€í•™)": (
                                        universities[i] if i < len(universities) else ""
                                    ),  # íŒŒì‹±ëœ ëŒ€í•™
                                    "ì†Œì†(êµ­ê°€)": (
                                        countries[i] if i < len(countries) else ""
                                    ),  # íŒŒì‹±ëœ êµ­ê°€
                                    "ë…¼ë¬¸ ë§í¬": (
                                        paper.get("link", "") if i == 0 else ""
                                    ),  # ì²« ë²ˆì§¸ ì €ìì—ë§Œ ë§í¬ í‘œì‹œ
                                }
                            )

                        # ë…¼ë¬¸ êµ¬ë¶„ì„ ìœ„í•œ ë¹ˆ í–‰ ì¶”ê°€
                        formatted_data.append(
                            {
                                "ë…¼ë¬¸ë²ˆí˜¸": "",
                                "ì €ì": "",
                                "ì´ë©”ì¼": "",
                                "ì†Œì†(ì „ê³µ)": "",
                                "ì†Œì†(ëŒ€í•™)": "",
                                "ì†Œì†(êµ­ê°€)": "",
                                "ë…¼ë¬¸ ë§í¬": "",
                            }
                        )

                    # DataFrame ìƒì„± ë° ì €ì¥
                    df = pd.DataFrame(formatted_data)

                    # ì‹œíŠ¸ ì´ë¦„ì—ì„œ íŠ¹ìˆ˜ ë¬¸ì ì œê±°
                    safe_keyword = re.sub(r"[^\w\s-]", "", keyword).strip()[
                        :31
                    ]  # Excel ì‹œíŠ¸ëª… ê¸¸ì´ ì œí•œ
                    df.to_excel(writer, sheet_name=safe_keyword, index=False)

        logger.info(f"ê²°ê³¼ê°€ {filename} íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def run(self):
        """ë©”ì¸ í¬ë¡¤ë§ ì‹¤í–‰"""
        try:
            print("ğŸš€ Scopus ë…¼ë¬¸ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤!")
            print(f"ğŸ“‹ ìˆ˜ì§‘í•  í‚¤ì›Œë“œ: {', '.join(self.keywords)}")
            print(f"ğŸ“„ ê° í‚¤ì›Œë“œë‹¹ ìµœëŒ€ {200}í˜ì´ì§€ í¬ë¡¤ë§")
            print(f"ğŸ“Š ë…¼ë¬¸ë³„ ë²ˆí˜¸ ë§¤ê¸°ê¸° ë° êµ¬ë¶„ì„ ìœ„í•œ ë¹ˆ í–‰ ì¶”ê°€")

            self.setup_driver()

            # ê³ ë ¤ëŒ€ ë„ì„œê´€ì—ì„œ Scopus ìë™ ì ‘ê·¼
            if not self.login_and_access_scopus():
                logger.error("Scopus ì ‘ê·¼ ì‹¤íŒ¨ë¡œ í¬ë¡¤ë§ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                return

            # Scopus ê²€ìƒ‰ í˜ì´ì§€ ì ‘ê·¼ í™•ì¸
            try:
                print("ğŸ” Scopus ê²€ìƒ‰ ê¸°ëŠ¥ í™•ì¸ ì¤‘...")
                WebDriverWait(self.driver, 15).until(
                    EC.presence_of_element_located(
                        (
                            By.CSS_SELECTOR,
                            "input[placeholder=' '][class*='styleguide-input_input']",
                        )
                    )
                )
                print("âœ… Scopus ê²€ìƒ‰ í˜ì´ì§€ ì ‘ê·¼ ì™„ë£Œ!")
            except:
                print("âŒ Scopus ê²€ìƒ‰ í˜ì´ì§€ ì ‘ê·¼ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")
                print("ìˆ˜ë™ìœ¼ë¡œ Scopus ê²€ìƒ‰ í˜ì´ì§€ë¡œ ì´ë™í•´ì£¼ì„¸ìš”.")
                input("ì¤€ë¹„ ì™„ë£Œ í›„ Enterë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”: ")

            # ê° í‚¤ì›Œë“œë³„ë¡œ í¬ë¡¤ë§ (ì‹œì‘ì ë¶€í„°)
            total_keywords = len(self.keywords)
            for idx, keyword in enumerate(
                self.keywords[self.start_keyword_index :], self.start_keyword_index + 1
            ):
                print(
                    f"\nğŸ” [{idx}/{total_keywords}] í‚¤ì›Œë“œ '{keyword}' í¬ë¡¤ë§ ì‹œì‘..."
                )
                print("ğŸ’¾ 5í˜ì´ì§€ë§ˆë‹¤ ìë™ ì €ì¥ë©ë‹ˆë‹¤.")
                print("ğŸ“Š ë…¼ë¬¸ë³„ ë²ˆí˜¸ì™€ êµ¬ë¶„ ë¹ˆ í–‰ì´ ìë™ìœ¼ë¡œ ì¶”ê°€ë©ë‹ˆë‹¤.")

                # ì²« ë²ˆì§¸ í‚¤ì›Œë“œë©´ ì§€ì •ëœ í˜ì´ì§€ë¶€í„°, ì•„ë‹ˆë©´ 1í˜ì´ì§€ë¶€í„°
                start_page = (
                    self.start_page if idx == self.start_keyword_index + 1 else 1
                )

                papers_data = self.crawl_pages(
                    keyword, max_pages=200, start_page=start_page
                )
                self.results_data[keyword] = papers_data

                print(f"âœ… í‚¤ì›Œë“œ '{keyword}' ì™„ë£Œ: {len(papers_data)}ê°œ ë…¼ë¬¸ ìˆ˜ì§‘")

                if idx < total_keywords:
                    print("â³ ë‹¤ìŒ í‚¤ì›Œë“œ í¬ë¡¤ë§ê¹Œì§€ 10-15ì´ˆ ëŒ€ê¸°...")
                    self.human_like_delay(10, 15)

            # ê²°ê³¼ ì €ì¥
            print(f"\nğŸ’¾ ê²°ê³¼ë¥¼ ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥ ì¤‘...")
            self.save_to_excel("scopus_papers_results.xlsx")

            # ìµœì¢… ê²°ê³¼ ìš”ì•½
            total_papers = sum(len(papers) for papers in self.results_data.values())
            print(f"\nğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
            print(f"ğŸ“Š ì´ ìˆ˜ì§‘ëœ ë…¼ë¬¸: {total_papers}ê°œ")
            print(f"ğŸ“ ì €ì¥ëœ íŒŒì¼: scopus_papers_results.xlsx")
            print(
                f"âœ¨ ê° ë…¼ë¬¸ë³„ë¡œ ë²ˆí˜¸ê°€ ë§¤ê²¨ì ¸ ìˆê³ , êµ¬ë¶„ì„ ìœ„í•œ ë¹ˆ í–‰ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤!"
            )

        except Exception as e:
            logger.error(f"í¬ë¡¤ë§ ì¤‘ ì „ì²´ ì˜¤ë¥˜: {str(e)}")
        finally:
            if self.driver:
                print("\nğŸ”’ ë¸Œë¼ìš°ì €ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤...")
                self.driver.quit()


# ì‹¤í–‰
if __name__ == "__main__":
    # ê¸°ë³¸ ì‹¤í–‰ (ì²˜ìŒë¶€í„° ì‹œì‘)
    crawler = ScopusCrawler()

    # ì¬ì‹œì‘ ì˜ˆì‹œ (ì£¼ì„ í•´ì œ í›„ ì‚¬ìš©):
    # crawler = ScopusCrawler(start_keyword_index=2, start_page=15)  # 3ë²ˆì§¸ í‚¤ì›Œë“œ, 15í˜ì´ì§€ë¶€í„°
    # crawler = ScopusCrawler(start_keyword_index=0, start_page=25)  # 1ë²ˆì§¸ í‚¤ì›Œë“œ, 25í˜ì´ì§€ë¶€í„°

    crawler.run()
