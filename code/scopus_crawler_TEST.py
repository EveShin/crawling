import requests
import pandas as pd
import time
import re
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.service import Service
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class ScopusCrawler:
    def __init__(self, start_keyword_index=0, start_page=1):
        """Scopus í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”"""
        # ì‹¤ì œ ìš´ì˜ìš©: í‚¤ì›Œë“œ 8ê°œ ì „ì²´
        self.keywords = [
            "LLM embodied", 
            "LLM AND IoT",
            "LLM wireless communications",
            "embodied AI AND IoT",
            "embodied AI internet of things",
            "LLM spectrum management",
            "embodied AI wireless communication",
            "LLM OR large language model"
        ]

        self.base_url = "https://www-scopus-com-ssl.oca.korea.ac.kr"
        self.library_url = "https://libs.korea.ac.kr/"  # ê³ ë ¤ëŒ€ ë„ì„œê´€ ì‚¬ì´íŠ¸
        self.driver = None
        self.results_data = {}

        # ì¬ì‹œì‘ ì„¤ì •
        self.start_keyword_index = start_keyword_index
        self.start_page = start_page

        print(
            f"ğŸ”„ ì‹œì‘ ì„¤ì •: í‚¤ì›Œë“œ {start_keyword_index + 1}ë²ˆì§¸ ('{self.keywords[start_keyword_index]}'), í˜ì´ì§€ {start_page}ë¶€í„°"
        )

    def setup_driver(self):
        """Chrome ë“œë¼ì´ë²„ ì„¤ì • (ë´‡ ê°ì§€ ìš°íšŒ ê°•í™”)"""
        chrome_options = Options()

        # ë´‡ ê°ì§€ ìš°íšŒë¥¼ ìœ„í•œ ê³ ê¸‰ ì„¤ì •
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option("useAutomationExtension", False)

        # ì‹¤ì œ ì‚¬ìš©ìì²˜ëŸ¼ ë³´ì´ê²Œ í•˜ëŠ” ì„¤ì •ë“¤
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-extensions")
        chrome_options.add_argument("--disable-plugins")
        chrome_options.add_argument(
            "--disable-images"
        )  # ì´ë¯¸ì§€ ë¡œë”© ë¹„í™œì„±í™”ë¡œ ì†ë„ í–¥ìƒ
        chrome_options.add_argument(
            "--disable-javascript"
        )  # ì¼ë¶€ ë´‡ ê°ì§€ ìŠ¤í¬ë¦½íŠ¸ ìš°íšŒ

        # ë¡œê·¸ ìˆ¨ê¸°ê¸°
        chrome_options.add_argument("--log-level=3")
        chrome_options.add_argument("--disable-logging")
        chrome_options.add_argument("--disable-background-networking")

        # ë” í˜„ì‹¤ì ì¸ User-Agent
        chrome_options.add_argument(
            "--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )

        # ìœˆë„ìš° í¬ê¸° ì„¤ì • (ë´‡ì€ ë³´í†µ headlessì´ë¯€ë¡œ ì‹¤ì œ í¬ê¸° ì„¤ì •)
        chrome_options.add_argument("--window-size=1920,1080")
        chrome_options.add_argument("--start-maximized")

        # ChromeDriver ìë™ ê´€ë¦¬
        service = Service(ChromeDriverManager().install())
        self.driver = webdriver.Chrome(service=service, options=chrome_options)

        # JavaScriptë¡œ webdriver ì†ì„± ìˆ¨ê¸°ê¸°
        self.driver.execute_script(
            "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})"
        )

        # ì¶”ê°€ ë´‡ ê°ì§€ ìš°íšŒ ìŠ¤í¬ë¦½íŠ¸
        self.driver.execute_cdp_cmd(
            "Page.addScriptToEvaluateOnNewDocument",
            {
                "source": """
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => undefined
                });
                
                Object.defineProperty(navigator, 'plugins', {
                    get: () => [1, 2, 3, 4, 5]
                });
                
                Object.defineProperty(navigator, 'languages', {
                    get: () => ['en-US', 'en', 'ko-KR', 'ko']
                });
                
                window.chrome = {
                    runtime: {}
                };
            """
            },
        )

        logger.info("Chrome ë“œë¼ì´ë²„ ì„¤ì • ì™„ë£Œ (ë´‡ ê°ì§€ ìš°íšŒ ê°•í™”)")

    def extract_author_affiliation_mapping(self):
        """'Show all information' íŒ¨ë„ ê¸°ì¤€ ì €ì-ì†Œì† ë§¤í•‘ ì •ë³´ ì¶”ì¶œ (ê°œì„ ëœ ë¡œì§)"""
        author_affiliation_map = {}
        affiliation_dict = {}

        try:
            # âœ… 1. ë¨¼ì € ì†Œì† ì •ë³´ ìˆ˜ì§‘ (ì²¨ì â†’ ì†Œì† í…ìŠ¤íŠ¸)
            print("ğŸ” ì†Œì† ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
            affiliation_elements = self.driver.find_elements(
                By.CSS_SELECTOR,
                "section[data-testid='detailed-information-affiliations'] ul.DetailedInformationFlyout_list__76Ipn li",
            )

            for aff in affiliation_elements:
                try:
                    # ì²¨ìê°€ ìˆëŠ” ê²½ìš°ì™€ ì—†ëŠ” ê²½ìš° ëª¨ë‘ ì²˜ë¦¬
                    try:
                        sup = aff.find_element(By.TAG_NAME, "sup").text.strip()
                        text = aff.find_element(By.TAG_NAME, "span").text.strip()
                        affiliation_dict[sup] = text
                        print(f"ğŸ›ï¸ [{sup}] {text}")
                    except NoSuchElementException:
                        # ì²¨ìê°€ ì—†ëŠ” ì†Œì†ì¸ ê²½ìš°
                        text = aff.text.strip()
                        if text:  # ë¹ˆ í…ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ê²½ìš°ë§Œ
                            # ê¸°ë³¸ í‚¤ë¡œ ì €ì¥ (ì²¨ì ì—†ìŒì„ ë‚˜íƒ€ëƒ„)
                            default_key = "default"
                            # ì´ë¯¸ defaultê°€ ìˆìœ¼ë©´ ë²ˆí˜¸ ì¶”ê°€
                            counter = 1
                            while default_key in affiliation_dict:
                                default_key = f"default{counter}"
                                counter += 1
                            affiliation_dict[default_key] = text
                            print(f"ğŸ›ï¸ [ì²¨ìì—†ìŒ] {text} â†’ í‚¤: {default_key}")
                except Exception as e:
                    print(f"âš ï¸ ì†Œì† íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
                    continue

            # âœ… 2. ì €ì ì •ë³´ ì¶”ì¶œ (ì €ìëª… â†’ ì²¨ì ëª©ë¡)
            print("ğŸ‘¥ ì €ì ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
            author_elements = self.driver.find_elements(
                By.CSS_SELECTOR,
                "ul.DetailedInformationFlyout_list__76Ipn li[data-testid='authorItem-button']",
            )

            # ğŸ“Š ì†Œì† ê°œìˆ˜ì— ë”°ë¥¸ ì²˜ë¦¬ ë¡œì§ ê²°ì •
            num_affiliations = len(affiliation_dict)
            print(f"ğŸ“Š ë°œê²¬ëœ ì†Œì† ìˆ˜: {num_affiliations}ê°œ")

            for i, author_el in enumerate(author_elements, 1):
                try:
                    name = author_el.find_element(
                        By.CSS_SELECTOR, "span.Button_text__0dddp"
                    ).text.strip()

                    # ì´ë©”ì¼ ìƒëµ í•„í„°ë§
                    if not name or name.lower() in ["authors"] or name.startswith("+"):
                        continue

                    superscripts = []
                    sup_elements = author_el.find_elements(
                        By.CSS_SELECTOR, "sup.AuthorList_affiliation__bTM3u"
                    )
                    for sup in sup_elements:
                        val = sup.text.strip()
                        for s in re.split(r"[,\s]+", val):
                            if s:
                                superscripts.append(s)

                    # ğŸ”„ ê°œì„ ëœ ì†Œì† í• ë‹¹ ë¡œì§
                    if num_affiliations == 1:
                        # ì†Œì†ì´ í•˜ë‚˜ë¿ì´ë©´ ëª¨ë“  ì €ìì—ê²Œ ë™ì¼í•˜ê²Œ ì ìš©
                        superscripts = list(affiliation_dict.keys())
                        print(f"ğŸ‘¤ {name} â†’ {superscripts} (ë‹¨ì¼ ì†Œì† - ëª¨ë“  ì €ì ê³µí†µ)")
                    elif num_affiliations > 1:
                        # ì†Œì†ì´ ì—¬ëŸ¬ ê°œì¸ ê²½ìš°
                        if not superscripts:
                            # ì²¨ìê°€ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ì†Œì†ì„ ê¸°ë³¸ìœ¼ë¡œ í• ë‹¹
                            first_affiliation = list(affiliation_dict.keys())[0] if affiliation_dict else ""
                            if first_affiliation:
                                superscripts = [first_affiliation]
                                print(f"ğŸ‘¤ {name} â†’ {superscripts} (ì²¨ì ì—†ìŒ - ê¸°ë³¸ ì†Œì† í• ë‹¹)")
                            else:
                                superscripts = []
                                print(f"ğŸ‘¤ {name} â†’ ì†Œì† ì—†ìŒ")
                        else:
                            print(f"ğŸ‘¤ {name} â†’ {superscripts} (ì²¨ì ë§¤í•‘)")
                    elif num_affiliations == 0:
                        # ì†Œì† ì •ë³´ê°€ ì•„ì˜ˆ ì—†ëŠ” ê²½ìš°
                        superscripts = []
                        print(f"ğŸ‘¤ {name} â†’ ì†Œì† ì •ë³´ ì „í˜€ ì—†ìŒ")
                    else:
                        # ì˜ˆì™¸ ìƒí™©
                        superscripts = []
                        print(f"ğŸ‘¤ {name} â†’ ì˜ˆì™¸ ìƒí™© - ì†Œì† ì—†ìŒ")

                    author_affiliation_map[name] = superscripts

                except Exception as e:
                    print(f"âŒ ì €ì {i} ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
                    continue

            # ğŸ“ˆ ìµœì¢… ê²°ê³¼ ìš”ì•½
            print(f"\nğŸ“ˆ ë§¤í•‘ ê²°ê³¼ ìš”ì•½:")
            print(f"   - ì´ ì†Œì† ìˆ˜: {len(affiliation_dict)}ê°œ")
            print(f"   - ì´ ì €ì ìˆ˜: {len(author_affiliation_map)}ëª…")
            
            # ì†Œì† ì—†ëŠ” ì €ì í™•ì¸
            authors_without_affiliation = [name for name, affs in author_affiliation_map.items() if not affs]
            if authors_without_affiliation:
                print(f"   âš ï¸ ì†Œì† ì—†ëŠ” ì €ì: {len(authors_without_affiliation)}ëª…")
                for name in authors_without_affiliation:
                    print(f"      - {name}")

        except Exception as e:
            print(f"âŒ ë§¤í•‘ ì „ì²´ ì‹¤íŒ¨: {str(e)}")

        return author_affiliation_map, affiliation_dict

    def contains_llm(self, text):
        """LLM ê´€ë ¨ í‚¤ì›Œë“œ íƒì§€ (ë‹¨ì–´ ê²½ê³„ ì‚¬ìš©)"""
        import re
        text_lower = text.lower()
        
        # 1. LLM ì•½ì–´ë“¤ (ë‹¨ì–´ ê²½ê³„ ì ìš©)
        if re.search(r'\bllms?\b', text_lower):
            return True
        
        # 2. ì „ì²´ ìš©ì–´
        if "large language model" in text_lower:
            return True
        
        return False

    def extract_llm_sentences(self, text):
        """LLMì´ í¬í•¨ëœ ë¬¸ì¥ë“¤ ì¶”ì¶œ"""
        import re
        
        # ë¬¸ì¥ ë¶„ë¦¬ (ì , ëŠë‚Œí‘œ, ë¬¼ìŒí‘œ ê¸°ì¤€)
        sentences = re.split(r'[.!?]+', text)
        llm_sentences = []
        
        for sentence in sentences:
            sentence = sentence.strip()
            if sentence and self.contains_llm(sentence):
                llm_sentences.append(sentence)
        
        return ' | '.join(llm_sentences)

    def human_like_delay(self, min_seconds=1, max_seconds=3):
        """ì‚¬ëŒì²˜ëŸ¼ ëœë¤í•œ ì§€ì—°ì‹œê°„"""
        import random

        delay = random.uniform(min_seconds, max_seconds)
        time.sleep(delay)

    def login_and_access_scopus(self):
        """ê³ ë ¤ëŒ€ ë„ì„œê´€ì—ì„œ Scopus ì ‘ê·¼ (ë¡œê·¸ì¸ ê³¼ì • ìƒëµ)"""
        try:
            # ê³ ë ¤ëŒ€ ë„ì„œê´€ ì‚¬ì´íŠ¸ë¡œ ì´ë™
            print("ğŸŒ ê³ ë ¤ëŒ€í•™êµ ë„ì„œê´€ ì‚¬ì´íŠ¸ë¡œ ì´ë™ ì¤‘...")
            self.driver.get(self.library_url)
            time.sleep(3)

            # ë°”ë¡œ "í•™ìˆ DB" ë²„íŠ¼ í´ë¦­ ì‹œë„
            print("ğŸ” í•™ìˆ DB ë©”ë‰´ë¥¼ í´ë¦­í•©ë‹ˆë‹¤...")
            try:
                academic_db_button = WebDriverWait(self.driver, 10).until(
                    EC.element_to_be_clickable(
                        (By.CSS_SELECTOR, "a[data-target='.home-service-link-group-2']")
                    )
                )
                academic_db_button.click()
                time.sleep(2)
                print("âœ… í•™ìˆ DB ë©”ë‰´ í´ë¦­ ì„±ê³µ!")
            except TimeoutException:
                print("âŒ í•™ìˆ DB ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                print("ğŸ“š ìˆ˜ë™ìœ¼ë¡œ í•™ìˆ DB ë©”ë‰´ë¥¼ í´ë¦­í•œ í›„ Enterë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”...")
                input("í•™ìˆ DB ë©”ë‰´ í´ë¦­ ì™„ë£Œ í›„ Enter: ")

            # Scopus ë§í¬ í´ë¦­
            print("ğŸ“– Scopus ë°ì´í„°ë² ì´ìŠ¤ì— ì ‘ê·¼í•©ë‹ˆë‹¤...")
            try:
                scopus_link = WebDriverWait(self.driver, 10).until(
                    EC.element_to_be_clickable(
                        (By.XPATH, "//a[contains(@href, 'scopus.com') and text()='Scopus']")
                    )
                )

                # í˜„ì¬ ì°½ì—ì„œ Scopus ë§í¬ ì£¼ì†Œ ê°€ì ¸ì˜¤ê¸°
                scopus_url = scopus_link.get_attribute("href")
                print(f"ğŸ”— Scopus í˜ì´ì§€ë¡œ ì´ë™í•©ë‹ˆë‹¤...")

                # Scopusë¡œ ì´ë™
                self.driver.get(scopus_url)
                time.sleep(5)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
                print("âœ… Scopus ì ‘ê·¼ ì„±ê³µ!")
                
            except TimeoutException:
                print("âŒ Scopus ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                print("ğŸ“– ìˆ˜ë™ìœ¼ë¡œ Scopus ë§í¬ë¥¼ í´ë¦­í•œ í›„ Enterë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”...")
                input("Scopus ì ‘ê·¼ ì™„ë£Œ í›„ Enter: ")

            # Scopus ë¡œê·¸ì¸ì´ í•„ìš”í•œ ê²½ìš°ì—ë§Œ ë¡œê·¸ì¸ ìš”ì²­
            print("\n" + "=" * 60)
            print("ğŸ” Scopus ì ‘ê·¼ í™•ì¸")
            print("=" * 60)
            print("1. Scopus í˜ì´ì§€ê°€ ë¡œë”©ë˜ì—ˆìŠµë‹ˆë‹¤")
            print("2. ë§Œì•½ ë¡œê·¸ì¸ì´ í•„ìš”í•˜ë‹¤ë©´ ë¡œê·¸ì¸ì„ ì™„ë£Œí•´ì£¼ì„¸ìš”")
            print("3. Scopus ê²€ìƒ‰ í˜ì´ì§€ì— ì ‘ê·¼ ê°€ëŠ¥í•˜ë©´ Enterë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”")
            print("=" * 60)
            input("Scopus ì‚¬ìš© ì¤€ë¹„ ì™„ë£Œ í›„ Enterë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”: ")

            print("âœ… Scopus ì ‘ê·¼ ì™„ë£Œ!")
            return True

        except Exception as e:
            logger.error(f"Scopus ì ‘ê·¼ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            print("âŒ ìë™ ì ‘ê·¼ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            print("ìˆ˜ë™ìœ¼ë¡œ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ì§„í–‰í•´ì£¼ì„¸ìš”:")
            print("1. ê³ ë ¤ëŒ€ ë„ì„œê´€ â†’ í•™ìˆ DB â†’ Scopus ì ‘ê·¼")
            print("2. í•„ìš”ì‹œ Scopus ë¡œê·¸ì¸ ì™„ë£Œ")
            input("ìˆ˜ë™ìœ¼ë¡œ Scopus ì ‘ê·¼ ì™„ë£Œ í›„ Enter: ")
            return True

    def set_results_per_page(self, count=10):  # ê¸°ë³¸ê°’ ìœ ì§€: 10ê°œ
        """í˜ì´ì§€ë‹¹ í‘œì‹œí•  ê²°ê³¼ ìˆ˜ ì„¤ì •"""
        try:
            # Display ë“œë¡­ë‹¤ìš´ ì°¾ê¸°
            display_select = WebDriverWait(self.driver, 5).until(
                EC.presence_of_element_located(
                    (By.CSS_SELECTOR, ".Select-module__vDMww")
                )
            )

            from selenium.webdriver.support.ui import Select

            select = Select(display_select)
            try:
                select.select_by_value("10")  # ê¸°ë³¸ 10ê°œ ì„¤ì •
            except:
                try:
                    select.select_by_value("20")  # 10ì´ ì—†ìœ¼ë©´ 20ê°œ
                except:
                    select.select_by_value("50")   # 20ë„ ì—†ìœ¼ë©´ 50ê°œ

            time.sleep(3)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            logger.info(f"í˜ì´ì§€ë‹¹ ê²°ê³¼ ì„¤ì • ì™„ë£Œ (ê¸°ë³¸ê°’ ì‚¬ìš©)")

        except Exception as e:
            logger.warning(f"ê²°ê³¼ í‘œì‹œ ê°œìˆ˜ ì„¤ì • ì‹¤íŒ¨: {str(e)}")
            print("âš ï¸ í˜ì´ì§€ë‹¹ ê²°ê³¼ ìˆ˜ ì„¤ì •ì„ ê±´ë„ˆëœë‹ˆë‹¤. ê¸°ë³¸ê°’ ì‚¬ìš©.")

    def search_keyword(self, keyword):
        """íŠ¹ì • í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ ì‹¤í–‰"""
        try:
            # í˜„ì¬ URL í™•ì¸ ë° ì •ë¦¬
            current_url = self.driver.current_url
            print(f"ğŸ”— í˜„ì¬ URL: {current_url}")

            # URL íŒŒì‹±í•˜ì—¬ ì˜¬ë°”ë¥¸ ê¸°ë³¸ URL êµ¬ì„±
            try:
                from urllib.parse import urlparse

                parsed_url = urlparse(current_url)

                # ê³ ë ¤ëŒ€ í”„ë¡ì‹œ URLë§Œ ì‚¬ìš©
                base_url = "https://www-scopus-com-ssl.oca.korea.ac.kr"

            except Exception as e:
                print(f"âŒ URL íŒŒì‹± ì˜¤ë¥˜: {e}")
                print(
                    "âš ï¸ Scopus í˜ì´ì§€ê°€ ì•„ë‹™ë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ Scopus ê²€ìƒ‰ í˜ì´ì§€ë¡œ ì´ë™í•´ì£¼ì„¸ìš”."
                )
                input("Scopus ê²€ìƒ‰ í˜ì´ì§€ ì ‘ê·¼ í›„ Enterë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”: ")
                base_url = "https://www-scopus-com-ssl.oca.korea.ac.kr"  # ê¸°ë³¸ê°’

            print(f"ğŸ”— ê¸°ë³¸ URL: {base_url}")

            # í˜„ì¬ í˜ì´ì§€ì—ì„œ ê²€ìƒ‰ í•„ë“œ ì°¾ê¸° ì‹œë„
            try:
                search_input = WebDriverWait(self.driver, 5).until(
                    EC.presence_of_element_located(
                        (
                            By.CSS_SELECTOR,
                            "input[placeholder=' '][class*='styleguide-input_input']",
                        )
                    )
                )
                print("âœ… í˜„ì¬ í˜ì´ì§€ì—ì„œ ê²€ìƒ‰ í•„ë“œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤!")

            except TimeoutException:
                # ê²€ìƒ‰ í˜ì´ì§€ë¡œ ì´ë™ ì‹œë„
                print("ğŸ” ê²€ìƒ‰ í˜ì´ì§€ë¡œ ì´ë™ì„ ì‹œë„í•©ë‹ˆë‹¤...")
                search_paths = [
                    "/search/form.uri?display=basic",
                    "/search/form.uri",
                    "/document/search.uri",
                ]

                for search_path in search_paths:
                    try:
                        search_url = base_url + search_path
                        print(f"ğŸ”— ì‹œë„í•˜ëŠ” URL: {search_url}")
                        self.driver.get(search_url)
                        time.sleep(3)

                        # ê²€ìƒ‰ í•„ë“œê°€ ìˆëŠ”ì§€ í™•ì¸
                        search_input = WebDriverWait(self.driver, 5).until(
                            EC.presence_of_element_located(
                                (
                                    By.CSS_SELECTOR,
                                    "input[placeholder=' '][class*='styleguide-input_input']",
                                )
                            )
                        )
                        print("âœ… ê²€ìƒ‰ í˜ì´ì§€ë¡œ ì´ë™ ì„±ê³µ!")
                        break
                    except Exception as e:
                        print(f"âŒ URL ì‹¤íŒ¨: {search_url} - {str(e)}")
                        continue
                else:
                    # ëª¨ë“  URL ì‹¤íŒ¨ì‹œ ìˆ˜ë™ ì´ë™ ìš”ì²­
                    print("âŒ ìë™ìœ¼ë¡œ ê²€ìƒ‰ í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    print("í˜„ì¬ í˜ì´ì§€ê°€ ì´ë¯¸ ê²€ìƒ‰ í˜ì´ì§€ë¼ë©´ ê·¸ëƒ¥ Enterë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
                    print(
                        "ì•„ë‹ˆë©´ ìˆ˜ë™ìœ¼ë¡œ Scopus ê²€ìƒ‰ í˜ì´ì§€ë¡œ ì´ë™í•œ í›„ Enterë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”."
                    )
                    input("Scopus ê²€ìƒ‰ í˜ì´ì§€ í™•ì¸ í›„ Enter: ")

                    # ë‹¤ì‹œ ê²€ìƒ‰ í•„ë“œ ì°¾ê¸° ì‹œë„
                    try:
                        search_input = WebDriverWait(self.driver, 10).until(
                            EC.presence_of_element_located(
                                (
                                    By.CSS_SELECTOR,
                                    "input[placeholder=' '][class*='styleguide-input_input']",
                                )
                            )
                        )
                    except:
                        print(
                            "âŒ ê²€ìƒ‰ í•„ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ ê²€ìƒ‰ì„ ì§„í–‰í•´ì£¼ì„¸ìš”."
                        )
                        print(f"ê²€ìƒ‰ì–´: {keyword}")
                        input("ìˆ˜ë™ ê²€ìƒ‰ ì™„ë£Œ í›„ Enter: ")
                        return True

            # ê²€ìƒ‰ì–´ ì…ë ¥
            print(f"ğŸ” ê²€ìƒ‰ì–´ ì…ë ¥: {keyword}")
            search_input.clear()
            self.human_like_delay(0.5, 1)  # íƒ€ì´í•‘ ì§€ì—°
            search_input.send_keys(keyword)

            # Search withinì„ Keywordsë¡œ ì„¤ì •
            try:
                search_within_dropdown = WebDriverWait(self.driver, 5).until(
                    EC.element_to_be_clickable(
                        (By.CSS_SELECTOR, "select[data-testid='select-search-within']")
                    )
                )

                from selenium.webdriver.support.ui import Select

                select = Select(search_within_dropdown)
                select.select_by_value("KEY")
                print("âœ… ê²€ìƒ‰ ë²”ìœ„ë¥¼ Keywordsë¡œ ì„¤ì •")
            except:
                print("âš ï¸ ê²€ìƒ‰ ë²”ìœ„ ì„¤ì •ì„ ê±´ë„ˆëœë‹ˆë‹¤.")

            # ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­
            print("ğŸ” ê²€ìƒ‰ ì‹¤í–‰...")
            self.human_like_delay(0.5, 1)  # í´ë¦­ ì „ ì§€ì—°
            search_button = WebDriverWait(self.driver, 5).until(
                EC.element_to_be_clickable(
                    (By.CSS_SELECTOR, "button[type='submit'][class*='Button_button']")
                )
            )
            search_button.click()

            # ê²°ê³¼ í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° (ì—¬ëŸ¬ selector ì‹œë„)
            result_selectors = [
                "tbody tr.TableItems-module__A6xTk",
                ".result-item",
                "[data-testid='search-results']",
                ".document-result",
                ".search-results-content",
            ]

            print("â³ ê²€ìƒ‰ ê²°ê³¼ ë¡œë”© ëŒ€ê¸°... (ìµœëŒ€ 15ì´ˆ)")
            for selector in result_selectors:
                try:
                    WebDriverWait(self.driver, 15).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, selector))
                    )
                    print(f"âœ… ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ ë¡œë”© ì™„ë£Œ!")
                    break
                except:
                    continue
            else:
                print("âš ï¸ ìë™ìœ¼ë¡œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                print("ê²€ìƒ‰ ê²°ê³¼ê°€ í‘œì‹œë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
                input("ê²€ìƒ‰ ê²°ê³¼ í™•ì¸ í›„ Enterë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”: ")

            # í˜ì´ì§€ë‹¹ ê²°ê³¼ ìˆ˜ ì„¤ì • (ê¸°ë³¸ê°’ ìœ ì§€)
            self.set_results_per_page(10)

            logger.info(f"í‚¤ì›Œë“œ '{keyword}' ê²€ìƒ‰ ì™„ë£Œ")
            return True

        except Exception as e:
            logger.error(f"í‚¤ì›Œë“œ '{keyword}' ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            print(f"âŒ ìë™ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            print("ìˆ˜ë™ìœ¼ë¡œ ë‹¤ìŒì„ ì§„í–‰í•´ì£¼ì„¸ìš”:")
            print(f"1. ê²€ìƒ‰ì–´ '{keyword}' ì…ë ¥")
            print("2. Keywords ë²”ìœ„ë¡œ ê²€ìƒ‰")
            print("3. ê²€ìƒ‰ ê²°ê³¼ í™•ì¸ í›„ Enter")
            input("ìˆ˜ë™ ê²€ìƒ‰ ì™„ë£Œ í›„ Enter: ")
            return True

    def extract_paper_links(self, paper_elements):  # ì‹¤ì œ ìš´ì˜: ëª¨ë“  ë…¼ë¬¸ ì²˜ë¦¬
        """ë…¼ë¬¸ ìš”ì†Œë“¤ì—ì„œ ìƒì„¸ í˜ì´ì§€ ë§í¬ ì¶”ì¶œ (ëª¨ë“  ë…¼ë¬¸)"""
        paper_links = []

        # ğŸ” ë””ë²„ê¹…: ëª¨ë“  ë…¼ë¬¸ ë§í¬ ì¶œë ¥
        print(f"ğŸ” í˜„ì¬ í˜ì´ì§€ì˜ ëª¨ë“  ë…¼ë¬¸ ({len(paper_elements)}ê°œ)")
        for i, element in enumerate(paper_elements):
            try:
                link = element.find_element(By.CSS_SELECTOR, "h3 a").get_attribute('href')
                print(f"ğŸ“„ ë…¼ë¬¸ {i+1}: {link}")
            except Exception as e:
                print(f"âŒ ë…¼ë¬¸ {i+1} ë§í¬ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")

        # ğŸš€ ì‹¤ì œ ìš´ì˜: ëª¨ë“  ë…¼ë¬¸ ì²˜ë¦¬
        for i, paper_element in enumerate(paper_elements, 1):
            try:
                # ë…¼ë¬¸ ìƒì„¸ í˜ì´ì§€ ë§í¬ ì¶”ì¶œ
                title_link = paper_element.find_element(By.CSS_SELECTOR, "h3 a")
                href = title_link.get_attribute("href")

                # ì ˆëŒ€ URLì¸ì§€ ìƒëŒ€ URLì¸ì§€ í™•ì¸
                if href.startswith("http"):
                    paper_link = href
                else:
                    # ìƒëŒ€ URLì¸ ê²½ìš° ê¸°ë³¸ URLê³¼ ê²°í•©
                    if "oca.korea.ac.kr" in self.driver.current_url:
                        base_url = "https://www-scopus-com-ssl.oca.korea.ac.kr"
                    else:
                        base_url = "https://www.scopus.com"
                    paper_link = base_url + href

                paper_links.append(paper_link)

            except Exception as e:
                logger.warning(f"ë…¼ë¬¸ ë§í¬ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                print(f"âŒ [{i}] ë…¼ë¬¸ ë§í¬ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
                continue

        print(f"ğŸ“‹ ì´ {len(paper_elements)}ê°œ ì¤‘ {len(paper_links)}ê°œ ë§í¬ ì¶”ì¶œ ì™„ë£Œ")
        return paper_links

    def get_detailed_author_info(self, paper_link):
        """ë…¼ë¬¸ ìƒì„¸ í˜ì´ì§€ì—ì„œ ì €ì ìƒì„¸ ì •ë³´ ì¶”ì¶œ (Show all information ë²„íŠ¼ í´ë¦­ í›„)"""
        detailed_info = {
            "authors": [],
            "emails": [],
            "detailed_affiliations": [],
            "raw_affiliations": [],  # ì›ë³¸ ì†Œì† ì •ë³´ (ì²¨ì í¬í•¨)
            "universities": [],
            "countries": [],
            "detected_sentences": "",  # ğŸ†• LLM íƒì§€ëœ ë¬¸ì¥ë“¤
            "link": paper_link,
        }

        try:
            # ìƒˆ íƒ­ì—ì„œ ìƒì„¸ í˜ì´ì§€ ì—´ê¸°
            self.driver.execute_script(f"window.open('{paper_link}', '_blank');")
            self.driver.switch_to.window(self.driver.window_handles[-1])

            # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° - ë” ì¶©ë¶„í•œ ì‹œê°„ê³¼ ëª…ì‹œì  ëŒ€ê¸°
            print("â³ ë…¼ë¬¸ ìƒì„¸ í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì¤‘...")
            self.human_like_delay(5, 7)  # ë” ê¸´ ì´ˆê¸° ëŒ€ê¸°

            # ğŸ” ì œëª©ê³¼ ì´ˆë¡ì—ì„œ LLM í‚¤ì›Œë“œ ê²€ì‚¬
            print("ğŸ” ì œëª©ê³¼ ì´ˆë¡ì—ì„œ LLM í‚¤ì›Œë“œ ê²€ì‚¬ ì¤‘...")
            
            # ì œëª© ì¶”ì¶œ - ëª…ì‹œì  ëŒ€ê¸° ì¶”ê°€
            title_text = ""
            try:
                print("ğŸ“ ì œëª© ì¶”ì¶œ ì‹œë„ ì¤‘...")
                title_element = WebDriverWait(self.driver, 15).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "h2[data-testid='publication-titles']"))
                )
                title_text = title_element.text.strip()
                print(f"ğŸ“ ì œëª©: {title_text}")
            except TimeoutException:
                print(f"âš ï¸ ì œëª© ì¶”ì¶œ ì‹œê°„ ì´ˆê³¼")
            except Exception as e:
                print(f"âš ï¸ ì œëª© ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")

            # ì´ˆë¡ ì¶”ì¶œ - ëª…ì‹œì  ëŒ€ê¸° ì¶”ê°€
            abstract_text = ""
            try:
                print("ğŸ“„ ì´ˆë¡ ì¶”ì¶œ ì‹œë„ ì¤‘...")
                abstract_element = WebDriverWait(self.driver, 15).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "div[id='document-details-abstract']"))
                )
                abstract_text = abstract_element.text.strip()
                print(f"ğŸ“„ ì´ˆë¡: {abstract_text[:100]}..." if len(abstract_text) > 100 else f"ğŸ“„ ì´ˆë¡: {abstract_text}")
            except TimeoutException:
                print(f"âš ï¸ ì´ˆë¡ ì¶”ì¶œ ì‹œê°„ ì´ˆê³¼")
            except Exception as e:
                print(f"âš ï¸ ì´ˆë¡ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")

            # LLM í‚¤ì›Œë“œ ê²€ì‚¬
            title_has_llm = self.contains_llm(title_text) if title_text else False
            abstract_has_llm = self.contains_llm(abstract_text) if abstract_text else False
            
            if not (title_has_llm or abstract_has_llm):
                print("âŒ LLM í‚¤ì›Œë“œê°€ ì œëª©ì´ë‚˜ ì´ˆë¡ì— ì—†ìŒ - ë…¼ë¬¸ ìŠ¤í‚µ")
                detailed_info["detected_sentences"] = "LLM í‚¤ì›Œë“œ ì—†ìŒ - ìŠ¤í‚µë¨"
                
                # íƒ­ ë‹«ê³  ì›ë˜ íƒ­ìœ¼ë¡œ ëŒì•„ê°€ê¸°
                self.driver.close()
                self.driver.switch_to.window(self.driver.window_handles[0])
                return detailed_info

            # LLM íƒì§€ëœ ê²½ìš°
            print("âœ… LLM í‚¤ì›Œë“œ íƒì§€ë¨ - ì €ì ì •ë³´ ìˆ˜ì§‘ ì§„í–‰")
            
            # íƒì§€ëœ ë¬¸ì¥ë“¤ ì¶”ì¶œ
            detected_sentences = []
            if title_has_llm:
                title_sentences = self.extract_llm_sentences(title_text)
                if title_sentences:
                    detected_sentences.append(f"ì œëª©: {title_sentences}")
            
            if abstract_has_llm:
                abstract_sentences = self.extract_llm_sentences(abstract_text)
                if abstract_sentences:
                    detected_sentences.append(f"ì´ˆë¡: {abstract_sentences}")
            
            detailed_info["detected_sentences"] = " | ".join(detected_sentences)

            # "Show all information" ë²„íŠ¼ í´ë¦­
            try:
                show_all_button = WebDriverWait(self.driver, 10).until(
                    EC.element_to_be_clickable(
                        (By.XPATH, "//button[.//span[text()='Show all information']]")
                    )
                )
                self.driver.execute_script("arguments[0].click();", show_all_button)
                print("ğŸ” 'Show all information' ë²„íŠ¼ í´ë¦­ ì„±ê³µ ")
                time.sleep(5)  # ì¶©ë¶„í•œ ì‹œê°ì  í™•ì¸ ë”œë ˆì´
            except Exception as e:
                print(f"âš ï¸ 'Show all information' ë²„íŠ¼ í´ë¦­ ì‹¤íŒ¨ ë˜ëŠ” ì—†ìŒ: {str(e)}")

            # âœ… 'Show all information' íŒ¨ë„ ê¸°ì¤€ ì¶”ì¶œ
            author_affiliation_map, affiliation_dict = (
                self.extract_author_affiliation_mapping()
            )

            for name, superscripts in author_affiliation_map.items():
                detailed_info["authors"].append(name)

                # ì´ë©”ì¼ (í•´ë‹¹ author block ë‚´ì— ìˆì„ ê²½ìš°)
                try:
                    email_element = self.driver.find_element(
                        By.XPATH,
                        f"//span[text()='{name}']/ancestor::li//a[starts-with(@href, 'mailto:')]",
                    )
                    email = email_element.get_attribute("href").replace("mailto:", "")
                    detailed_info["emails"].append(email)
                except:
                    detailed_info["emails"].append("")

                # ì†Œì† ë§¤í•‘ - ê° ì €ìë³„ë¡œ í•´ë‹¹í•˜ëŠ” ì†Œì†ë§Œ
                affs, affs_raw, univs, countries = [], [], [], []
                for sup in superscripts:
                    if sup in affiliation_dict:
                        aff_text = affiliation_dict[sup]
                        affs.append(aff_text)
                        
                        # ì›ë³¸ í‘œì‹œ: ì²¨ìê°€ defaultë¥˜ë©´ [ì²¨ìì—†ìŒ], ì•„ë‹ˆë©´ [ì²¨ì]
                        if sup.startswith("default"):
                            affs_raw.append(f"[ì²¨ìì—†ìŒ] {aff_text}")
                        else:
                            affs_raw.append(f"[{sup}] {aff_text}")
                            
                        parsed = self.parse_affiliation(aff_text)
                        univs.append(parsed["university"])
                        countries.append(parsed["country"])

                if not affs:
                    affs = [""]
                    affs_raw = [""]  # ì†Œì† ì—†ëŠ” ê²½ìš° ë¹ˆê°’
                    univs = [""]
                    countries = [""]

                detailed_info["detailed_affiliations"].append(" | ".join(affs))
                detailed_info["raw_affiliations"].append(" | ".join(affs_raw))  # í•´ë‹¹ ì €ìì˜ ì†Œì†ë§Œ
                detailed_info["universities"].append(" | ".join(univs))
                detailed_info["countries"].append(" | ".join(countries))

            self.driver.close()
            self.driver.switch_to.window(self.driver.window_handles[0])

        except Exception as e:
            logger.error(f"ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            try:
                if len(self.driver.window_handles) > 1:
                    self.driver.close()
                self.driver.switch_to.window(self.driver.window_handles[0])
            except:
                pass

        return detailed_info

    def parse_affiliation(self, affiliation_text):
        """ì†Œì† ì •ë³´ë¥¼ ì „ê³µ, ëŒ€í•™, êµ­ê°€ë¡œ ë¶„ë¦¬"""
        parsed = {"department": "", "university": "", "country": ""}

        if not affiliation_text:
            return parsed

        # ì‰¼í‘œë¡œ ë¶„ë¦¬
        parts = [part.strip() for part in affiliation_text.split(",")]

        if len(parts) >= 3:
            # ì¼ë°˜ì ì¸ í˜•íƒœ: Department, University, Country
            parsed["department"] = parts[0]
            parsed["university"] = parts[1]
            parsed["country"] = parts[-1]  # ë§ˆì§€ë§‰ì´ êµ­ê°€

        elif len(parts) == 2:
            # University, Country í˜•íƒœ
            parsed["university"] = parts[0]
            parsed["country"] = parts[1]

        elif len(parts) == 1:
            # ì „ì²´ë¥¼ ëŒ€í•™ìœ¼ë¡œ ê°„ì£¼
            parsed["university"] = parts[0]

        # University í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì§€ ì•Šì€ ì²« ë²ˆì§¸ partëŠ” í•™ê³¼ë¡œ ê°„ì£¼
        if (
            len(parts) >= 2
            and "university" not in parts[0].lower()
            and "college" not in parts[0].lower()
        ):
            for i, part in enumerate(parts[1:], 1):
                if any(
                    keyword in part.lower()
                    for keyword in ["university", "college", "institute", "school"]
                ):
                    parsed["department"] = parts[0]
                    parsed["university"] = part
                    if i + 1 < len(parts):
                        parsed["country"] = parts[-1]
                    break

        print(f"ğŸ“ ì†Œì† ë¶„ì„: {affiliation_text}")
        print(f"   - ì „ê³µ: {parsed['department']}")
        print(f"   - ëŒ€í•™: {parsed['university']}")
        print(f"   - êµ­ê°€: {parsed['country']}")

        return parsed

    def save_batch_results(
        self, keyword, papers_data, start_page, end_page, paper_start_index=1
    ):
        """ë°°ì¹˜ ê²°ê³¼ ì €ì¥ (5í˜ì´ì§€ì”©) - ì‹¤ì œ ìš´ì˜ìš©"""
        try:
            # ì•ˆì „í•œ íŒŒì¼ëª… ìƒì„±
            safe_keyword = re.sub(r"[^\w\s-]", "", keyword).replace(" ", "_")
            filename = f"scopus_{safe_keyword}_pages_{start_page}-{end_page}.xlsx"

            if papers_data:
                # ë°ì´í„° ì •ë¦¬
                formatted_data = []
                current_paper_index = paper_start_index

                for paper in papers_data:
                    authors = paper.get("authors", [""])
                    emails = paper.get("emails", [""])
                    affiliations = paper.get("detailed_affiliations", [""])
                    raw_affiliations = paper.get("raw_affiliations", [""])  # ì›ë³¸ ì†Œì†
                    universities = paper.get("universities", [""])
                    countries = paper.get("countries", [""])
                    detected_sentences = paper.get("detected_sentences", "")  # ğŸ†• íƒì§€ëœ ë¬¸ì¥

                    max_len = max(
                        len(authors),
                        len(emails),
                        len(affiliations),
                        len(raw_affiliations),  # ì›ë³¸ ì†Œì† ê¸¸ì´ë„ ê³ ë ¤
                        len(universities),
                        len(countries),
                    )
                    authors.extend([""] * (max_len - len(authors)))
                    emails.extend([""] * (max_len - len(emails)))
                    affiliations.extend([""] * (max_len - len(affiliations)))
                    raw_affiliations.extend([""] * (max_len - len(raw_affiliations)))
                    universities.extend([""] * (max_len - len(universities)))
                    countries.extend([""] * (max_len - len(countries)))

                    for i in range(max_len):
                        formatted_data.append(
                            {
                                "ë…¼ë¬¸ë²ˆí˜¸": (
                                    current_paper_index if i == 0 else ""
                                ),  # ì²« ë²ˆì§¸ ì €ìì—ë§Œ ë…¼ë¬¸ë²ˆí˜¸ í‘œì‹œ
                                "ì €ì": authors[i] if i < len(authors) else "",
                                "ì´ë©”ì¼": emails[i] if i < len(emails) else "",
                                "ì†Œì†(ì›ë³¸)": (
                                    raw_affiliations[i] if i < len(raw_affiliations) else ""
                                ),  # ì›ë³¸ ì†Œì† (ì²¨ì í¬í•¨)
                                "ì†Œì†(ì „ê³µ)": (
                                    affiliations[i] if i < len(affiliations) else ""
                                ),
                                "ì†Œì†(ëŒ€í•™)": (
                                    universities[i] if i < len(universities) else ""
                                ),
                                "ì†Œì†(êµ­ê°€)": (
                                    countries[i] if i < len(countries) else ""
                                ),
                                "íƒì§€ë¬¸ì¥": (
                                    detected_sentences if i == 0 else ""
                                ),  # ğŸ†• ì²« ë²ˆì§¸ ì €ìì—ë§Œ íƒì§€ë¬¸ì¥ í‘œì‹œ
                                "ë…¼ë¬¸ ë§í¬": (
                                    paper.get("link", "") if i == 0 else ""
                                ),  # ì²« ë²ˆì§¸ ì €ìì—ë§Œ ë§í¬ í‘œì‹œ
                            }
                        )

                    # ë…¼ë¬¸ êµ¬ë¶„ì„ ìœ„í•œ ë¹ˆ í–‰ ì¶”ê°€
                    formatted_data.append(
                        {
                            "ë…¼ë¬¸ë²ˆí˜¸": "",
                            "ì €ì": "",
                            "ì´ë©”ì¼": "",
                            "ì†Œì†(ì›ë³¸)": "",
                            "ì†Œì†(ì „ê³µ)": "",
                            "ì†Œì†(ëŒ€í•™)": "",
                            "ì†Œì†(êµ­ê°€)": "",
                            "íƒì§€ë¬¸ì¥": "",  # ğŸ†•
                            "ë…¼ë¬¸ ë§í¬": "",
                        }
                    )

                    current_paper_index += 1

                df = pd.DataFrame(formatted_data)
                df.to_excel(filename, index=False)
                print(
                    f"ğŸ’¾ ë°°ì¹˜ ì €ì¥ ì™„ë£Œ: {filename} ({len(papers_data)}ê°œ ë…¼ë¬¸, {start_page}-{end_page}í˜ì´ì§€)"
                )

                return current_paper_index  # ë‹¤ìŒ ë…¼ë¬¸ ë²ˆí˜¸ ë°˜í™˜

        except Exception as e:
            print(f"âŒ ë°°ì¹˜ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            return paper_start_index

    def crawl_pages(self, keyword, max_pages=3, start_page=1):  # ğŸ§ª í…ŒìŠ¤íŠ¸ìš©: 3í˜ì´ì§€ë¡œ ì œí•œ
        """íŠ¹ì • í‚¤ì›Œë“œì— ëŒ€í•´ ì—¬ëŸ¬ í˜ì´ì§€ í¬ë¡¤ë§ - í…ŒìŠ¤íŠ¸ìš©"""
        papers_data = []

        # ë…¼ë¬¸ ë²ˆí˜¸ ì‹œì‘ê°’ ê³„ì‚° (í˜ì´ì§€ë‹¹ í‰ê·  ë…¼ë¬¸ ìˆ˜ë¥¼ ê³ ë ¤)
        paper_index = ((start_page - 1) * 2) + 1  # í˜ì´ì§€ë‹¹ 2ê°œ ë…¼ë¬¸

        # ì²« ê²€ìƒ‰ ì‹¤í–‰
        if not self.search_keyword(keyword):
            return papers_data

        # ì‹œì‘ í˜ì´ì§€ê°€ 1ì´ ì•„ë‹ˆë©´ í•´ë‹¹ í˜ì´ì§€ë¡œ ì´ë™
        if start_page > 1:
            print(f"ğŸ”„ í˜ì´ì§€ {start_page}ë¡œ ì´ë™ ì¤‘...")
            success = self.navigate_to_page(start_page)
            if not success:
                print(f"âŒ í˜ì´ì§€ {start_page}ë¡œ ì´ë™ ì‹¤íŒ¨. í˜ì´ì§€ 1ë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.")
                start_page = 1
                paper_index = 1

        for page_num in range(start_page, max_pages + 1):
            try:
                logger.info(
                    f"í‚¤ì›Œë“œ '{keyword}' - í˜ì´ì§€ {page_num}/{max_pages} í…ŒìŠ¤íŠ¸ í¬ë¡¤ë§ ì¤‘..."
                )

                # í˜„ì¬ í˜ì´ì§€ì˜ ë…¼ë¬¸ ë§í¬ë“¤ ìˆ˜ì§‘
                paper_elements = WebDriverWait(self.driver, 10).until(
                    EC.presence_of_all_elements_located(
                        (By.CSS_SELECTOR, "tbody tr.TableItems-module__A6xTk")
                    )
                )

                if not paper_elements:
                    logger.info(f"í˜ì´ì§€ {page_num}ì—ì„œ ë” ì´ìƒ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    break

                print(f"ğŸ“‹ í˜ì´ì§€ {page_num}: {len(paper_elements)}ê°œ ë…¼ë¬¸ ë°œê²¬ (í…ŒìŠ¤íŠ¸: ì²˜ìŒ 2ê°œë§Œ ì²˜ë¦¬)")

                # ë…¼ë¬¸ ë§í¬ë“¤ ì¶”ì¶œ (ì‹¤ì œ ìš´ì˜: ëª¨ë“  ë…¼ë¬¸)
                paper_links = self.extract_paper_links(paper_elements)  # limit íŒŒë¼ë¯¸í„° ì œê±°

                # ê° ë…¼ë¬¸ ìƒì„¸ í˜ì´ì§€ì—ì„œ ì €ì ì •ë³´ ì¶”ì¶œ
                page_papers = []  # ğŸ”§ ë³€ìˆ˜ ì´ˆê¸°í™” ì¶”ê°€
                
                for j, paper_link in enumerate(paper_links, 1):
                    print(
                        f"ğŸ” [{j}/{len(paper_links)}] ë…¼ë¬¸ ìƒì„¸ ì •ë³´ í™•ì¸ ì¤‘..."
                    )

                    # ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (LLM í•„í„°ë§ í¬í•¨)
                    detailed_info = self.get_detailed_author_info(paper_link)
                    detailed_info["link"] = paper_link

                    # LLM í‚¤ì›Œë“œê°€ íƒì§€ëœ ê²½ìš°ë§Œ ë…¼ë¬¸ ë²ˆí˜¸ ì¦ê°€
                    if detailed_info.get("detected_sentences") != "LLM í‚¤ì›Œë“œ ì—†ìŒ - ìŠ¤í‚µë¨":
                        detailed_info["paper_number"] = paper_index
                        print(
                            f"âœ… ë…¼ë¬¸ {paper_index}ë²ˆ: ì €ì {len(detailed_info.get('authors', []))}ëª…, ì´ë©”ì¼ {len(detailed_info.get('emails', []))}ê°œ ìˆ˜ì§‘"
                        )
                        paper_index += 1  # LLM íƒì§€ëœ ë…¼ë¬¸ë§Œ ë²ˆí˜¸ ì¦ê°€
                    else:
                        detailed_info["paper_number"] = "none"
                        print(f"â­ï¸ ë…¼ë¬¸ ìŠ¤í‚µ: LLM í‚¤ì›Œë“œ ì—†ìŒ")

                    page_papers.append(detailed_info)
                    papers_data.append(detailed_info)

                    self.human_like_delay(2, 4)

                print(f"ğŸ“„ í˜ì´ì§€ {page_num} ì™„ë£Œ: {len(page_papers)}ê°œ ë…¼ë¬¸ ìˆ˜ì§‘")

                # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
                if page_num < max_pages:
                    try:
                        # ğŸ”§ í˜„ì¬ URL ì €ì¥ (í˜ì´ì§€ ì´ë™ í™•ì¸ìš©)
                        current_url = self.driver.current_url
                        print(f"ğŸ”— í˜„ì¬ URL: {current_url}")
                        
                        next_button = self.driver.find_element(
                            By.XPATH, "//button[.//span[text()='Next']]"
                        )

                        if next_button.is_enabled() and not next_button.get_attribute(
                            "disabled"
                        ):
                            print("ğŸ”„ Next ë²„íŠ¼ í´ë¦­ ì¤‘...")
                            next_button.click()
                            self.human_like_delay(3, 5)
                            
                            # ğŸ”§ URLì´ ì‹¤ì œë¡œ ë°”ë€Œì—ˆëŠ”ì§€ í™•ì¸
                            new_url = self.driver.current_url
                            print(f"ğŸ”— ìƒˆ URL: {new_url}")
                            
                            if current_url == new_url:
                                print("âŒ URLì´ ë°”ë€Œì§€ ì•ŠìŒ - í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨")
                                # ê°•ì œë¡œ ìƒˆ í˜ì´ì§€ ìš”ì†Œ ëŒ€ê¸°
                                try:
                                    WebDriverWait(self.driver, 10).until(
                                        EC.staleness_of(paper_elements[0])  # ì´ì „ ìš”ì†Œê°€ ì‚¬ë¼ì§ˆ ë•Œê¹Œì§€ ëŒ€ê¸°
                                    )
                                    print("âœ… í˜ì´ì§€ ìš”ì†Œ ê°±ì‹  í™•ì¸")
                                except:
                                    print("âš ï¸ í˜ì´ì§€ ìš”ì†Œ ê°±ì‹  í™•ì¸ ì‹¤íŒ¨")
                            else:
                                print("âœ… URL ë³€ê²½ í™•ì¸ - í˜ì´ì§€ ì´ë™ ì„±ê³µ")
                            
                            logger.info(f"í˜ì´ì§€ {page_num + 1}ë¡œ ì´ë™")
                        else:
                            logger.info("ë” ì´ìƒ ë‹¤ìŒ í˜ì´ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
                            break

                    except NoSuchElementException:
                        logger.info(
                            "Next ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ í˜ì´ì§€ì¸ ê²ƒ ê°™ìŠµë‹ˆë‹¤."
                        )
                        break
                    except Exception as e:
                        logger.error(f"ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                        break

            except Exception as e:
                logger.error(f"í˜ì´ì§€ {page_num} í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                continue

        # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥
        if papers_data:
            self.save_batch_results(keyword, papers_data, start_page, page_num, 1)

        logger.info(f"í‚¤ì›Œë“œ '{keyword}' í…ŒìŠ¤íŠ¸ í¬ë¡¤ë§ ì™„ë£Œ: {len(papers_data)}ê°œ ë…¼ë¬¸")
        return papers_data

    def navigate_to_page(self, target_page):
        """íŠ¹ì • í˜ì´ì§€ë¡œ ì´ë™"""
        try:
            print(f"ğŸ“„ í˜ì´ì§€ {target_page}ë¡œ ì´ë™ ì¤‘...")

            # ê°„ë‹¨í•œ ë°©ë²•: target_page - 1ë²ˆ Next ë²„íŠ¼ í´ë¦­
            for i in range(target_page - 1):
                try:
                    next_button = self.driver.find_element(
                        By.XPATH, "//button[.//span[text()='Next']]"
                    )
                    if next_button.is_enabled():
                        next_button.click()
                        self.human_like_delay(2, 3)
                        print(f"  -> í˜ì´ì§€ {i + 2}ë¡œ ì´ë™")
                    else:
                        print(f"âŒ í˜ì´ì§€ {i + 2}ë¡œ ì´ë™ ì‹¤íŒ¨")
                        return False
                except:
                    print(f"âŒ í˜ì´ì§€ {i + 2}ë¡œ ì´ë™ ì¤‘ ì˜¤ë¥˜")
                    return False

            print(f"âœ… í˜ì´ì§€ {target_page} ë„ì°©")
            return True

        except Exception as e:
            print(f"âŒ í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨: {str(e)}")
            return False

    def save_progress(self, keyword, next_page):
        """ì§„í–‰ ìƒí™© ì €ì¥"""
        try:
            progress_info = {
                "keyword": keyword,
                "keyword_index": self.keywords.index(keyword),
                "next_page": next_page,
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            }

            with open("scopus_progress.txt", "w", encoding="utf-8") as f:
                f.write(f"ë§ˆì§€ë§‰ ì™„ë£Œ: {progress_info['keyword']}\n")
                f.write(f"í‚¤ì›Œë“œ ë²ˆí˜¸: {progress_info['keyword_index']}\n")
                f.write(f"ë‹¤ìŒ ì‹œì‘ í˜ì´ì§€: {progress_info['next_page']}\n")
                f.write(f"ì‹œê°„: {progress_info['timestamp']}\n")
                f.write(f"\nì¬ì‹œì‘ ë°©ë²•:\n")
                f.write(
                    f"crawler = ScopusCrawler(start_keyword_index={progress_info['keyword_index']}, start_page={progress_info['next_page']})\n"
                )

        except Exception as e:
            print(f"âŒ ì§„í–‰ ìƒí™© ì €ì¥ ì‹¤íŒ¨: {str(e)}")

    def save_to_excel(self, filename="scopus_papers_results.xlsx"):
        """ê²°ê³¼ë¥¼ ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥ - ì‹¤ì œ ìš´ì˜ìš©"""
        with pd.ExcelWriter(filename, engine="openpyxl") as writer:
            for keyword, papers_data in self.results_data.items():
                if papers_data:
                    # ë°ì´í„° ì •ë¦¬
                    formatted_data = []

                    for paper_index, paper in enumerate(papers_data, 1):
                        # ì €ìë³„ë¡œ í–‰ ë¶„ë¦¬
                        authors = paper.get("authors", [""])
                        emails = paper.get("emails", [""])
                        detailed_affiliations = paper.get(
                            "detailed_affiliations", [""]
                        )  # íŒŒì‹±ëœ ì†Œì†
                        raw_affiliations = paper.get("raw_affiliations", [""])  # ì›ë³¸ ì†Œì†
                        universities = paper.get("universities", [""])  # íŒŒì‹±ëœ ëŒ€í•™
                        countries = paper.get("countries", [""])  # íŒŒì‹±ëœ êµ­ê°€
                        detected_sentences = paper.get("detected_sentences", "")  # ğŸ†• íƒì§€ëœ ë¬¸ì¥

                        # ë¦¬ìŠ¤íŠ¸ ê¸¸ì´ ë§ì¶”ê¸°
                        max_len = max(
                            len(authors),
                            len(emails),
                            len(detailed_affiliations),
                            len(raw_affiliations),  # ì›ë³¸ ì†Œì† ê¸¸ì´ë„ ê³ ë ¤
                            len(universities),
                            len(countries),
                        )
                        authors.extend([""] * (max_len - len(authors)))
                        emails.extend([""] * (max_len - len(emails)))
                        detailed_affiliations.extend(
                            [""] * (max_len - len(detailed_affiliations))
                        )
                        raw_affiliations.extend([""] * (max_len - len(raw_affiliations)))
                        universities.extend([""] * (max_len - len(universities)))
                        countries.extend([""] * (max_len - len(countries)))

                        for i in range(max_len):
                            formatted_data.append(
                                {
                                    "ë…¼ë¬¸ë²ˆí˜¸": (
                                        paper_index if i == 0 else ""
                                    ),  # ì²« ë²ˆì§¸ ì €ìì—ë§Œ ë…¼ë¬¸ë²ˆí˜¸ í‘œì‹œ
                                    "ì €ì": authors[i] if i < len(authors) else "",
                                    "ì´ë©”ì¼": emails[i] if i < len(emails) else "",
                                    "ì†Œì†(ì›ë³¸)": (
                                        raw_affiliations[i] if i < len(raw_affiliations) else ""
                                    ),  # ì›ë³¸ ì†Œì† (ì²¨ì í¬í•¨)
                                    "ì†Œì†(ì „ê³µ)": (
                                        detailed_affiliations[i]
                                        if i < len(detailed_affiliations)
                                        else ""
                                    ),  # íŒŒì‹±ëœ ì†Œì†
                                    "ì†Œì†(ëŒ€í•™)": (
                                        universities[i] if i < len(universities) else ""
                                    ),  # íŒŒì‹±ëœ ëŒ€í•™
                                    "ì†Œì†(êµ­ê°€)": (
                                        countries[i] if i < len(countries) else ""
                                    ),  # íŒŒì‹±ëœ êµ­ê°€
                                    "íƒì§€ë¬¸ì¥": (
                                        detected_sentences if i == 0 else ""
                                    ),  # ğŸ†• ì²« ë²ˆì§¸ ì €ìì—ë§Œ íƒì§€ë¬¸ì¥ í‘œì‹œ
                                    "ë…¼ë¬¸ ë§í¬": (
                                        paper.get("link", "") if i == 0 else ""
                                    ),  # ì²« ë²ˆì§¸ ì €ìì—ë§Œ ë§í¬ í‘œì‹œ
                                }
                            )

                        # ë…¼ë¬¸ êµ¬ë¶„ì„ ìœ„í•œ ë¹ˆ í–‰ ì¶”ê°€
                        formatted_data.append(
                            {
                                "ë…¼ë¬¸ë²ˆí˜¸": "",
                                "ì €ì": "",
                                "ì´ë©”ì¼": "",
                                "ì†Œì†(ì›ë³¸)": "",
                                "ì†Œì†(ì „ê³µ)": "",
                                "ì†Œì†(ëŒ€í•™)": "",
                                "ì†Œì†(êµ­ê°€)": "",
                                "íƒì§€ë¬¸ì¥": "",  # ğŸ†•
                                "ë…¼ë¬¸ ë§í¬": "",
                            }
                        )

                    # DataFrame ìƒì„± ë° ì €ì¥
                    df = pd.DataFrame(formatted_data)

                    # ì‹œíŠ¸ ì´ë¦„ì—ì„œ íŠ¹ìˆ˜ ë¬¸ì ì œê±°
                    safe_keyword = re.sub(r"[^\w\s-]", "", keyword).strip()[
                        :31
                    ]  # Excel ì‹œíŠ¸ëª… ê¸¸ì´ ì œí•œ
                    df.to_excel(writer, sheet_name=safe_keyword, index=False)

        logger.info(f"ê²°ê³¼ê°€ {filename} íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def run(self):
        """ë©”ì¸ í¬ë¡¤ë§ ì‹¤í–‰ - ì‹¤ì œ ìš´ì˜ìš©"""
        try:
            print("ğŸš€ Scopus ë…¼ë¬¸ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤!")
            print(f"ğŸ“‹ ìˆ˜ì§‘í•  í‚¤ì›Œë“œ: {', '.join(self.keywords)} (ì´ {len(self.keywords)}ê°œ)")
            print(f"ğŸ“„ ê° í‚¤ì›Œë“œë‹¹ ìµœëŒ€ 200í˜ì´ì§€, ëª¨ë“  ë…¼ë¬¸ í¬ë¡¤ë§")
            print(f"ğŸ¯ LLM ê´€ë ¨ ë…¼ë¬¸ë§Œ ì„ ë³„ì ìœ¼ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")

            self.setup_driver()

            # ê³ ë ¤ëŒ€ ë„ì„œê´€ì—ì„œ Scopus ìë™ ì ‘ê·¼ (ë¡œê·¸ì¸ ê³¼ì • ê°„ì†Œí™”)
            if not self.login_and_access_scopus():
                logger.error("Scopus ì ‘ê·¼ ì‹¤íŒ¨ë¡œ í¬ë¡¤ë§ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                return

            # Scopus ê²€ìƒ‰ í˜ì´ì§€ ì ‘ê·¼ í™•ì¸
            try:
                print("ğŸ” Scopus ê²€ìƒ‰ ê¸°ëŠ¥ í™•ì¸ ì¤‘...")
                WebDriverWait(self.driver, 15).until(
                    EC.presence_of_element_located(
                        (
                            By.CSS_SELECTOR,
                            "input[placeholder=' '][class*='styleguide-input_input']",
                        )
                    )
                )
                print("âœ… Scopus ê²€ìƒ‰ í˜ì´ì§€ ì ‘ê·¼ ì™„ë£Œ!")
            except:
                print("âŒ Scopus ê²€ìƒ‰ í˜ì´ì§€ ì ‘ê·¼ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")
                print("ìˆ˜ë™ìœ¼ë¡œ Scopus ê²€ìƒ‰ í˜ì´ì§€ë¡œ ì´ë™í•´ì£¼ì„¸ìš”.")
                input("ì¤€ë¹„ ì™„ë£Œ í›„ Enterë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”: ")

            # ê° í‚¤ì›Œë“œë³„ë¡œ í¬ë¡¤ë§ (ì‹œì‘ì ë¶€í„°)
            total_keywords = len(self.keywords)
            for idx, keyword in enumerate(
                self.keywords[self.start_keyword_index :], self.start_keyword_index + 1
            ):
                print(
                    f"\nğŸ” [{idx}/{total_keywords}] í‚¤ì›Œë“œ '{keyword}' í¬ë¡¤ë§ ì‹œì‘..."
                )
                print("ğŸ’¾ 5í˜ì´ì§€ë§ˆë‹¤ ìë™ ì €ì¥ë©ë‹ˆë‹¤.")
                print("ğŸ“Š ë…¼ë¬¸ë³„ ë²ˆí˜¸ì™€ êµ¬ë¶„ ë¹ˆ í–‰ì´ ìë™ìœ¼ë¡œ ì¶”ê°€ë©ë‹ˆë‹¤.")

                # ì²« ë²ˆì§¸ í‚¤ì›Œë“œë©´ ì§€ì •ëœ í˜ì´ì§€ë¶€í„°, ì•„ë‹ˆë©´ 1í˜ì´ì§€ë¶€í„°
                start_page = (
                    self.start_page if idx == self.start_keyword_index + 1 else 1
                )

                papers_data = self.crawl_pages(
                    keyword, max_pages=200, start_page=start_page
                )
                self.results_data[keyword] = papers_data

                print(f"âœ… í‚¤ì›Œë“œ '{keyword}' ì™„ë£Œ: {len(papers_data)}ê°œ ë…¼ë¬¸ ìˆ˜ì§‘")

                if idx < total_keywords:
                    print("â³ ë‹¤ìŒ í‚¤ì›Œë“œ í¬ë¡¤ë§ê¹Œì§€ 10-15ì´ˆ ëŒ€ê¸°...")
                    self.human_like_delay(10, 15)

            # ê²°ê³¼ ì €ì¥
            print(f"\nğŸ’¾ ê²°ê³¼ë¥¼ ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥ ì¤‘...")
            self.save_to_excel("scopus_papers_results.xlsx")

            # ìµœì¢… ê²°ê³¼ ìš”ì•½
            total_papers = sum(len(papers) for papers in self.results_data.values())
            print(f"\nğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
            print(f"ğŸ“Š ì´ ìˆ˜ì§‘ëœ ë…¼ë¬¸: {total_papers}ê°œ")
            print(f"ğŸ“ ì €ì¥ëœ íŒŒì¼: scopus_papers_results.xlsx")
            print(f"âœ¨ ê° ë…¼ë¬¸ë³„ë¡œ ë²ˆí˜¸ê°€ ë§¤ê²¨ì ¸ ìˆê³ , êµ¬ë¶„ì„ ìœ„í•œ ë¹ˆ í–‰ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤!")

        except Exception as e:
            logger.error(f"í¬ë¡¤ë§ ì¤‘ ì „ì²´ ì˜¤ë¥˜: {str(e)}")
        finally:
            if self.driver:
                print("\nğŸ”’ ë¸Œë¼ìš°ì €ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤...")
                self.driver.quit()


# ì‹¤í–‰
if __name__ == "__main__":
    print("ğŸš€ Scopus í¬ë¡¤ëŸ¬ ì‹¤ì œ ìš´ì˜ ë²„ì „")
    print("=" * 50)
    print("ğŸ“‹ ìš´ì˜ ì„¤ì •:")
    print("   - í‚¤ì›Œë“œ: 8ê°œ")
    print("   - í˜ì´ì§€: 200í˜ì´ì§€")
    print("   - ë…¼ë¬¸: ëª¨ë“  ë…¼ë¬¸ (LLM ê´€ë ¨ë§Œ ì„ ë³„)")
    print("   - ì´ ì˜ˆìƒ ë…¼ë¬¸ ìˆ˜: ìˆ˜ì²œ~ìˆ˜ë§Œê°œ")
    print("=" * 50)
    
    # ê¸°ë³¸ ì‹¤í–‰ (ì²˜ìŒë¶€í„° ì‹œì‘)
    crawler = ScopusCrawler()
    
    # ì¬ì‹œì‘ ì˜ˆì‹œ (ì£¼ì„ í•´ì œ í›„ ì‚¬ìš©):
    # crawler = ScopusCrawler(start_keyword_index=2, start_page=15)  # 3ë²ˆì§¸ í‚¤ì›Œë“œ, 15í˜ì´ì§€ë¶€í„°
    # crawler = ScopusCrawler(start_keyword_index=0, start_page=25)  # 1ë²ˆì§¸ í‚¤ì›Œë“œ, 25í˜ì´ì§€ë¶€í„°
    
    crawler.run()